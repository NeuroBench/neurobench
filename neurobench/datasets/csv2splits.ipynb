{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, List, Tuple\n",
    "from pathlib import Path\n",
    "import os\n",
    "from torch import Tensor\n",
    "from torchaudio.datasets.utils import _load_waveform\n",
    "import torch\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "SAMPLE_RATE = 48000\n",
    "ALL_LANGUAGES = [\"en\"] #, \"es\"]\n",
    "FOLDER_AUDIO = \"clips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"//scratch/p306982/data/fscil/FSCIL_subset/\"\n",
    "base_languages = ['en', 'fr', 'ca', 'de', 'rw']\n",
    "inc_languages = ['fa', 'eo', 'pt', 'eu', 'pl', 'cy', 'nl', 'ru', 'es', 'it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dict = {}\n",
    "writer_dict = {}\n",
    "for lang in inc_languages:\n",
    "    f = open(os.path.join(root, lang, 'evaluation.csv'), 'w')\n",
    "    files_dict[lang] = f\n",
    "    writer = csv.writer(files_dict[lang])\n",
    "    writer.writerow(['LINK', 'WORD', 'LANGUAGE'])\n",
    "    writer_dict[lang] = writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_word_count = {}\n",
    "for lang in inc_languages:\n",
    "    inc_word_count[lang] = {}\n",
    "    words = os.listdir(os.path.join(root, lang, 'clips'))\n",
    "    for word in words:\n",
    "        inc_word_count[lang][word]=0\n",
    "        clips = os.listdir(os.path.join(root, lang, 'clips', word))\n",
    "        for clip in clips:\n",
    "            if inc_word_count[lang][word] <200:\n",
    "                path = os.path.join(word, clip)\n",
    "                writer_dict[lang].writerow([path, word, lang])\n",
    "                inc_word_count[lang][word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in inc_languages:\n",
    "    files_dict[lang].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_f = open(os.path.join(root, 'base_train.csv'), 'w')\n",
    "base_val_f = open(os.path.join(root, 'base_val.csv'), 'w')\n",
    "base_test_f = open(os.path.join(root, 'base_test.csv'), 'w')\n",
    "evaluation_f = open(os.path.join(root, 'evaluation.csv'), 'w')\n",
    "writer_base_train = csv.writer(base_train_f)\n",
    "writer_base_val = csv.writer(base_val_f)\n",
    "writer_base_test = csv.writer(base_test_f)\n",
    "writer_evaluation = csv.writer(evaluation_f)\n",
    "header = ['LINK', 'WORD', 'LANGUAGE']\n",
    "writer_base_train.writerow(header)\n",
    "writer_base_val.writerow(header)\n",
    "writer_base_test.writerow(header)\n",
    "writer_evaluation.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_f.close()\n",
    "base_val_f.close()\n",
    "base_test_f.close()\n",
    "evaluation_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accept/test.opus'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('accept', 'test.opus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS = []\n",
    "languages = base_languages + inc_languages\n",
    "for lang in languages:\n",
    "    words = os.listdir(os.path.join(root, lang, 'clips'))\n",
    "    ALL_WORDS += words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_word_count = {}\n",
    "for lang in base_languages:\n",
    "    base_word_count[lang] = {}\n",
    "    words = os.listdir(os.path.join(root, lang, 'clips'))\n",
    "    for word in words:\n",
    "        base_word_count[lang][word]={'train': 0, 'val': 0, 'test': 0}\n",
    "        clips = os.listdir(os.path.join(root, lang, 'clips', word))\n",
    "        for clip in clips:\n",
    "            path = os.path.join(word, clip)\n",
    "            if base_word_count[lang][word]['train'] <500:\n",
    "                writer_base_train.writerow([path, word, lang])\n",
    "                base_word_count[lang][word]['train'] +=1\n",
    "            elif base_word_count[lang][word]['val'] <100:\n",
    "                writer_base_val.writerow([path, word, lang])\n",
    "                base_word_count[lang][word]['val'] +=1\n",
    "            elif base_word_count[lang][word]['test'] <100:\n",
    "                writer_base_test.writerow([path, word, lang])\n",
    "                base_word_count[lang][word]['test'] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in base_word_count:\n",
    "    for word in base_word_count[lang]:\n",
    "        if base_word_count[lang][word] != {'train': 500, 'val': 100, 'test': 100}:\n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'میخواستم': 200,\n",
       " 'امیدوارم': 200,\n",
       " 'تلویزیون': 200,\n",
       " 'آپارتمان': 200,\n",
       " 'بینالمللی': 200,\n",
       " 'بنابراین': 200,\n",
       " 'اسفندیار': 200,\n",
       " 'پرسپولیس': 200,\n",
       " 'بفرمایید': 200,\n",
       " 'میتوانند': 200}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mswc_fscil_splits(root: Union[str, Path], \n",
    "                               languages: List[str] = None, \n",
    "                               visualize: Optional[bool] = False):\n",
    "    \"\"\"\n",
    "    Generate new MSWC split for a few-shot class-incremental (FSCIL) learning scenario with the following split.\n",
    "    100 base classes with 500 train, 100 validation and 100 test samples each.\n",
    "    100 evaluation classes with 200 samples each (to use in a 10 sessions of 10 way set-up with N shots support to train on per class and the rest as a query to evaluate performance).\n",
    "    The 200 classes are arbitrarily chosen as common voice command words.\n",
    "    The base ones are then the 100 of these with the most clips (at least 700) per sample and the evaluation ones as the 100 following ones.\n",
    "\n",
    "    Args\n",
    "        root (str): Path of MSWC dataset folder where the Metadata.json file and en/ folders should be.\n",
    "        languages (List[str]): List of languages to use. Not implemented for now, only english will be used.\n",
    "        visualize (bool): Plots Word Clouds with library wordcloud for a visualization of the FSCIL keywords.\n",
    "\n",
    "    Returns: base_keywords, evaluation keywords (dictionarries)\n",
    "    They represent the number of available samples per respective keyword in the original MSWC dataset (although the number is then clipped as detailed above).\n",
    "    \"\"\"\n",
    "\n",
    "    base_keywords, evaluation_keywords = get_command_keywords(root, visualize=visualize)\n",
    "\n",
    "    if languages is None:\n",
    "        languages = ['en']\n",
    "\n",
    "    print(languages)\n",
    "    if languages  != ['en']:\n",
    "        print('Other languages than english are not supported yet.')\n",
    "\n",
    "    base_train_count = dict.fromkeys(base_keywords, 0) #{'train':0, 'val':0, 'test':0})\n",
    "    base_test_count = dict.fromkeys(base_keywords, 0)\n",
    "    base_val_count = dict.fromkeys(base_keywords, 0)\n",
    "    evaluation_count = dict.fromkeys(evaluation_keywords, 0)\n",
    "\n",
    "    for lang in languages:\n",
    "        base_train_f = open(os.path.join(root, 'base_train.csv'), 'w')\n",
    "        base_val_f = open(os.path.join(root, 'base_val.csv'), 'w')\n",
    "        base_test_f = open(os.path.join(root, 'base_test.csv'), 'w')\n",
    "        evaluation_f = open(os.path.join(root, 'evaluation.csv'), 'w')\n",
    "        writer_base_train = csv.writer(base_train_f)\n",
    "        writer_base_val = csv.writer(base_val_f)\n",
    "        writer_base_test = csv.writer(base_test_f)\n",
    "        writer_evaluation = csv.writer(evaluation_f)\n",
    "        header = ['LINK', 'WORD', 'VALID', 'SPEAKER', 'GENDER']\n",
    "        writer_base_train.writerow(header)\n",
    "        writer_base_val.writerow(header)\n",
    "        writer_base_test.writerow(header)\n",
    "        writer_evaluation.writerow(header)\n",
    "\n",
    "        with open(os.path.join(root, lang,  f'{lang}_splits.csv'), 'r') as f:\n",
    "            for line in f:\n",
    "                set, path, word, valid, speaker, gender = line.strip().split(',')\n",
    "                \n",
    "                # Skip header\n",
    "                if set == \"SET\":\n",
    "                    continue  \n",
    "\n",
    "                ### Successively assign samples to train (500), validation (100) and test (100) set\n",
    "                if word in base_keywords:\n",
    "                    if base_train_count[word] <500:\n",
    "                        writer_base_train.writerow([path, word, valid, speaker, gender])\n",
    "                        base_train_count[word] +=1\n",
    "                    elif base_val_count[word] <100:\n",
    "                        writer_base_val.writerow([path, word, valid, speaker, gender])\n",
    "                        base_val_count[word] +=1\n",
    "                    elif base_test_count[word] <100:\n",
    "                        writer_base_test.writerow([path, word, valid, speaker, gender])\n",
    "                        base_test_count[word] +=1\n",
    "\n",
    "                elif word in evaluation_keywords:\n",
    "                    if evaluation_count[word] <200:\n",
    "                        writer_evaluation.writerow([path, word, valid, speaker, gender])\n",
    "                        evaluation_count[word] +=1\n",
    "\n",
    "\n",
    "    base_train_f.close()\n",
    "    base_val_f.close()\n",
    "    base_test_f.close()\n",
    "    evaluation_f.close()\n",
    "\n",
    "    return base_keywords, evaluation_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
