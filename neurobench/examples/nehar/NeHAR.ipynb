{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyNLu56m20JM"
      },
      "source": [
        "# **Neuromorphic Human Activity Recognition (NeHAR) task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1yo7vAs8z3n"
      },
      "source": [
        "In this notebook we benchmarked SNN-based models performing the Neuromorphic Human Activity Recognition (NeHAR) task using IMU sensor data acquired form a commercial smartwatch.\n",
        "\n",
        "Human Activity Recognition (HAR) is a time-dependent task that has applications in various aspects of human life, from healthcare to sports, safety, and smart environments. In this task, we present a comparative analysis of different SNN-based models designed for classifying raw signals (Accelerometer and Gyroscope) collected in the Wireless Sensor Data Mining (WISDM) dataset.\n",
        "\n",
        "The WISDM dataset consists of data from 51 subjects performing 18 activities. This dataset collects signals from both the accelerometer and the gyroscope of a smartphone and a smartwatch. Each activity is recorded for 3 minutes with an acquisition rate of 20 Hz. The dataset's classes are balanced, with each activity represented in the dataset contributing approximately 5.3% to 5.8% of the total approximately 15.63 million samples.\n",
        "From the whole smartwatch dataset, we selected a subset of general hand-oriented activities for our analysis. These activities include: (1) dribbling in basketball, (2) playing catch with a tennis ball, (3) typing, (4) writing, (5) clapping, (6) brushing teeth, and (7) folding clothes. We divided the signals into non-overlapping temporal windows with a length of 2 seconds. These temporal windows serve as the input layer for the benchmarked models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt1kUxe6c_-s"
      },
      "source": [
        "---\n",
        "\n",
        "Refferring to the paper: Fra, V., Forno, E., Pignari, R., Stewart, T. C., Macii, E., & Urgese, G. (2022).\n",
        "***Human activity recognition: suitability of a neuromorphic approach for on-edge AIoT applications. Neuromorphic Computing and Engineering***, 2(1), 014006.\n",
        "DOI ***10.1088/2634-4386/ac4c38***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJsVu0H2FIol"
      },
      "source": [
        "## Environment set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuyCkI5mB1Qc"
      },
      "source": [
        "### Install packages in the Google Colab runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-qowl4U2xh2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gdown\n",
        "!pip install hyperopt\n",
        "!pip install matplotlib\n",
        "!pip install neurobench\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install scipy\n",
        "!pip install seaborn\n",
        "!pip install snntorch\n",
        "!pip install torch\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc4eYiEoCchd"
      },
      "source": [
        "### Basic import\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HiLoKFukCfHl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import random\n",
        "#from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from neurobench.models import SNNTorchModel\n",
        "from neurobench.accumulators.accumulator import aggregate, choose_max_count\n",
        "from neurobench.benchmarks import Benchmark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0pq1QdFDEH9"
      },
      "source": [
        "### Some utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hNEkKo98DGXk"
      },
      "outputs": [],
      "source": [
        "def create_directory(\n",
        "    directory_path\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Muller-Cleve, Simon F.; Istituto Italiano di Tecnologia - IIT; Event-driven perception in robotics - EDPR; Genova, Italy.\n",
        "    \"\"\"\n",
        "    if os.path.exists(directory_path):\n",
        "        return None\n",
        "    else:\n",
        "        try:\n",
        "            os.makedirs(directory_path)\n",
        "        except:\n",
        "            # in case another machine created the path meanwhile! :(\n",
        "            return None\n",
        "        return directory_path\n",
        "\n",
        "\n",
        "def train_validation_test_split(\n",
        "    data,\n",
        "    label,\n",
        "    split=[70, 20, 10],\n",
        "    seed=None,\n",
        "    multiple=False,\n",
        "    save_dataset=False,\n",
        "    save_tensor=False,\n",
        "    labels_type=None,\n",
        "    labels_mapping=None,\n",
        "    save_name=None,\n",
        "    save_path=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Creates train-validation-test splits using the sklearn train_test_split() twice.\n",
        "    Can be used either to prepare \"ready-to-use\" splits or to create and store splits.\n",
        "\n",
        "    If multiple splits are not needed and no saving option is set, the lists x_train, y_train, x_val, y_val, x_test, y_test are returned (without labels mapping).\n",
        "\n",
        "    Function accepts lists, arrays, and tensor.\n",
        "    Default split: [training: 70, validation: 20, test: 10]\n",
        "\n",
        "    Fra, Vittorio; Politecnico di Torino; EDA Group; Torino, Italy.\n",
        "    Muller-Cleve, Simon F.; Istituto Italiano di Tecnologia - IIT; Event-driven perception in robotics - EDPR; Genova, Italy.\n",
        "    \"\"\"\n",
        "\n",
        "    if multiple:\n",
        "        if (not save_dataset) & (not save_tensor):\n",
        "            raise ValueError(\"Multiple train-val splits are created but no saving option is enabled.\")\n",
        "\n",
        "    if save_dataset | save_tensor:\n",
        "        if (save_path == None) | (save_name == None):\n",
        "            raise ValueError(\"Check a file name and a path are provided to save the datasets.\")\n",
        "        filename_prefix = save_path + save_name\n",
        "        create_directory(save_path)\n",
        "\n",
        "    # do some sanity checks first\n",
        "    if len(split) != 3:\n",
        "        raise ValueError(\n",
        "            f\"Split dimensions are wrong. Expected 3 but got {len(split)}. Please provide split in the form [train size, test size, validation size].\")\n",
        "    if min(split) == 0.0:\n",
        "        raise ValueError(\n",
        "            \"Found entry 0.0. If you want to use only perfrom a two-folded split, use the sklearn train_test_split function only please.\")\n",
        "    if sum(split) > 99.0:\n",
        "        split = [x/100 for x in split]\n",
        "    if sum(split) < 0.99:\n",
        "        raise ValueError(\"Please use a split summing up to 1, or 100%.\")\n",
        "\n",
        "    train, val, test = split\n",
        "    split_1 = test\n",
        "    split_2 = 1 - train/(train+val)\n",
        "\n",
        "    x_trainval, x_test, y_trainval, y_test = train_test_split(\n",
        "        data, label, test_size=split_1, shuffle=True, stratify=label, random_state=seed)\n",
        "\n",
        "\n",
        "    if save_dataset: # Save the test split\n",
        "        filename_test = filename_prefix + \"_test\"\n",
        "        # xs test\n",
        "        with open(f\"{filename_test}.pkl\", 'wb') as handle:\n",
        "            pkl.dump(np.array(x_test, dtype=object), handle,\n",
        "                        protocol=pkl.HIGHEST_PROTOCOL)\n",
        "        # ys test\n",
        "        with open(f\"{filename_test}_label.pkl\", 'wb') as handle:\n",
        "            pkl.dump(np.array(y_test, dtype=object), handle,\n",
        "                        protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "    if save_tensor: # Save the test split\n",
        "        filename_test = filename_prefix + \"_ds_test\"\n",
        "        x_test = torch.as_tensor(np.array(x_test), dtype=torch.float)\n",
        "        if labels_type == str:\n",
        "            labels_test = torch.as_tensor(value2index(\n",
        "                y_test, labels_mapping), dtype=torch.long)\n",
        "        else:\n",
        "            labels_test = torch.as_tensor(y_test, dtype=torch.long)\n",
        "        ds_test = TensorDataset(x_test, labels_test)\n",
        "        torch.save(ds_test, \"{}.pt\".format(filename_test))\n",
        "\n",
        "    if multiple:\n",
        "\n",
        "        for ii in range(10):\n",
        "\n",
        "            x_train, x_val, y_train, y_val = train_test_split(\n",
        "                x_trainval, y_trainval, test_size=split_2, shuffle=True, stratify=y_trainval, random_state=seed)\n",
        "\n",
        "            if save_dataset:\n",
        "\n",
        "                filename_train = filename_prefix + \"_train\"\n",
        "                filename_val = filename_prefix + \"_val\"\n",
        "\n",
        "                # xs training\n",
        "                with open(f\"{filename_train}_{ii}.pkl\", 'wb') as handle:\n",
        "                    pkl.dump(np.array(x_train, dtype=object), handle,\n",
        "                                protocol=pkl.HIGHEST_PROTOCOL)\n",
        "                # ys training\n",
        "                with open(f\"{filename_train}_{ii}_label.pkl\", 'wb') as handle:\n",
        "                    pkl.dump(np.array(y_train, dtype=object), handle,\n",
        "                                protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "                # xs validation\n",
        "                with open(f\"{filename_val}_{ii}.pkl\", 'wb') as handle:\n",
        "                    pkl.dump(np.array(x_val, dtype=object), handle,\n",
        "                                protocol=pkl.HIGHEST_PROTOCOL)\n",
        "                # ys validation\n",
        "                with open(f\"{filename_val}_{ii}_label.pkl\", 'wb') as handle:\n",
        "                    pkl.dump(np.array(y_val, dtype=object), handle,\n",
        "                                protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "            if save_tensor:\n",
        "\n",
        "                filename_train = filename_prefix + \"_ds_train\"\n",
        "                filename_val = filename_prefix + \"_ds_val\"\n",
        "\n",
        "                x_train = torch.as_tensor(np.array(x_train), dtype=torch.float)\n",
        "                if labels_type == str:\n",
        "                    labels_train = torch.as_tensor(value2index(\n",
        "                        y_train, labels_mapping), dtype=torch.long)\n",
        "                else:\n",
        "                    labels_train = torch.as_tensor(y_train, dtype=torch.long)\n",
        "\n",
        "                x_validation = torch.as_tensor(\n",
        "                    np.array(x_val), dtype=torch.float)\n",
        "                if labels_type == str:\n",
        "                    labels_validation = torch.as_tensor(value2index(\n",
        "                        y_val, labels_mapping), dtype=torch.long)\n",
        "                else:\n",
        "                    labels_validation = torch.as_tensor(y_val, dtype=torch.long)\n",
        "\n",
        "                ds_train = TensorDataset(x_train, labels_train)\n",
        "                ds_val = TensorDataset(x_validation, labels_validation)\n",
        "\n",
        "                torch.save(ds_train, \"{}_{}.pt\".format(filename_train,ii))\n",
        "                torch.save(ds_val, \"{}_{}.pt\".format(filename_val,ii))\n",
        "\n",
        "    else:\n",
        "\n",
        "        x_train, x_val, y_train, y_val = train_test_split(\n",
        "            x_trainval, y_trainval, test_size=split_2, shuffle=True, stratify=y_trainval, random_state=seed)\n",
        "\n",
        "        if save_dataset:\n",
        "\n",
        "            filename_train = filename_prefix + \"_train\"\n",
        "            filename_val = filename_prefix + \"_val\"\n",
        "\n",
        "            # xs training\n",
        "            with open(f\"{filename_train}.pkl\", 'wb') as handle:\n",
        "                pkl.dump(np.array(x_train, dtype=object), handle,\n",
        "                            protocol=pkl.HIGHEST_PROTOCOL)\n",
        "            # ys training\n",
        "            with open(f\"{filename_train}_label.pkl\", 'wb') as handle:\n",
        "                pkl.dump(np.array(y_train, dtype=object), handle,\n",
        "                            protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "            # xs validation\n",
        "            with open(f\"{filename_val}.pkl\", 'wb') as handle:\n",
        "                pkl.dump(np.array(x_val, dtype=object), handle,\n",
        "                            protocol=pkl.HIGHEST_PROTOCOL)\n",
        "            # ys validation\n",
        "            with open(f\"{filename_val}_label.pkl\", 'wb') as handle:\n",
        "                pkl.dump(np.array(y_val, dtype=object), handle,\n",
        "                            protocol=pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "        if save_tensor:\n",
        "\n",
        "            filename_train = filename_prefix + \"_ds_train\"\n",
        "            filename_val = filename_prefix + \"_ds_val\"\n",
        "\n",
        "            x_train = torch.as_tensor(np.array(x_train), dtype=torch.float)\n",
        "            if labels_type == str:\n",
        "                labels_train = torch.as_tensor(value2index(\n",
        "                    y_train, labels_mapping), dtype=torch.long)\n",
        "            else:\n",
        "                labels_train = torch.as_tensor(y_train, dtype=torch.long)\n",
        "\n",
        "            x_validation = torch.as_tensor(\n",
        "                np.array(x_val), dtype=torch.float)\n",
        "            if labels_type == str:\n",
        "                labels_validation = torch.as_tensor(value2index(\n",
        "                    y_val, labels_mapping), dtype=torch.long)\n",
        "            else:\n",
        "                labels_validation = torch.as_tensor(y_val, dtype=torch.long)\n",
        "\n",
        "            ds_train = TensorDataset(x_train, labels_train)\n",
        "            ds_val = TensorDataset(x_validation, labels_validation)\n",
        "\n",
        "            torch.save(ds_train, filename_train)\n",
        "            torch.save(ds_val, filename_val)\n",
        "\n",
        "        return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "\n",
        "def value2key(\n",
        "    entry,\n",
        "    dictionary\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Fra, Vittorio; Politecnico di Torino; EDA Group; Torino, Italy.\n",
        "    \"\"\"\n",
        "\n",
        "    if (type(entry) != list) & (type(entry) != np.ndarray):\n",
        "\n",
        "        key = [list(dictionary.keys())[list(dictionary.values()).index(entry)]]\n",
        "\n",
        "    else:\n",
        "\n",
        "        key = [list(dictionary.keys())[list(dictionary.values()).index(e)] for e in entry]\n",
        "\n",
        "    return key\n",
        "\n",
        "\n",
        "def index2key(\n",
        "    entry,\n",
        "    dictionary\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Fra, Vittorio; Politecnico di Torino; EDA Group; Torino, Italy.\n",
        "    \"\"\"\n",
        "\n",
        "    if (type(entry) != list) & (type(entry) != np.ndarray):\n",
        "\n",
        "        key = [list(dictionary.keys())[entry]]\n",
        "\n",
        "    else:\n",
        "\n",
        "        key = [list(dictionary.keys())[e] for e in entry]\n",
        "\n",
        "    return key\n",
        "\n",
        "\n",
        "def value2index(\n",
        "    entry,\n",
        "    dictionary\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Fra, Vittorio; Politecnico di Torino; EDA Group; Torino, Italy.\n",
        "    \"\"\"\n",
        "\n",
        "    if (type(entry) != list) & (type(entry) != np.ndarray):\n",
        "\n",
        "        idx = [list(dictionary.values()).index(entry)]\n",
        "\n",
        "    else:\n",
        "\n",
        "        idx = [list(dictionary.values()).index(e) for e in entry]\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "def index2value(\n",
        "    entry,\n",
        "    dictionary\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Fra, Vittorio; Politecnico di Torino; EDA Group; Torino, Italy.\n",
        "    \"\"\"\n",
        "\n",
        "    if (type(entry) != list) & (type(entry) != np.ndarray):\n",
        "\n",
        "        value = [list(dictionary.values())[entry]]\n",
        "\n",
        "    else:\n",
        "\n",
        "        value = [list(dictionary.values())[e] for e in entry]\n",
        "\n",
        "    return value\n",
        "\n",
        "\n",
        "def training_loop(\n",
        "    dataset,\n",
        "    batch_size,\n",
        "    net,\n",
        "    optimizer,\n",
        "    loss_fn,\n",
        "    device):\n",
        "    \"\"\"\n",
        "    Fra, Vittorio; Politecnico di Torino; EDA Group; Torino, Italy.\n",
        "    \"\"\"\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    batch_loss = []\n",
        "    batch_acc = []\n",
        "\n",
        "    for data, labels in tqdm(train_loader):\n",
        "\n",
        "      data = data#.to(device).swapaxes(1, 0)\n",
        "      labels = labels#.to(device)\n",
        "\n",
        "      net.train()\n",
        "      rec = net.single_forward(data)\n",
        "      spk_rec = rec[0]\n",
        "\n",
        "      # Training loss\n",
        "      loss_val = loss_fn(spk_rec, labels)\n",
        "      batch_loss.append(loss_val.detach().cpu().item())\n",
        "\n",
        "      # Training accuracy\n",
        "      act_total_out = torch.sum(spk_rec, 0)  # sum over time\n",
        "      _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
        "      batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
        "\n",
        "      # Gradient calculation + weight update\n",
        "      optimizer.zero_grad()\n",
        "      loss_val.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    epoch_loss = np.mean(batch_loss)\n",
        "    epoch_acc = np.mean(batch_acc)\n",
        "\n",
        "    return [epoch_loss, epoch_acc]\n",
        "\n",
        "\n",
        "def val_test_loop(\n",
        "    dataset,\n",
        "    batch_size,\n",
        "    net,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    shuffle=True,\n",
        "    label_probabilities=False,\n",
        "    return_spikes=False):\n",
        "    \"\"\"\n",
        "    Fra, Vittorio; Politecnico di Torino; EDA Group; Torino, Italy.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "      net.eval()\n",
        "\n",
        "      loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
        "\n",
        "      batch_loss = []\n",
        "      batch_acc = []\n",
        "\n",
        "      for data, labels in tqdm(loader):\n",
        "          data = data#.to(device).swapaxes(1, 0)\n",
        "          labels = labels#.to(device)\n",
        "\n",
        "          rec = net.single_forward(data)\n",
        "          spk_out = rec[0]\n",
        "\n",
        "          # Loss\n",
        "          loss_val = loss_fn(spk_out, labels)\n",
        "          batch_loss.append(loss_val.detach().cpu().item())\n",
        "\n",
        "          # Accuracy\n",
        "          act_total_out = torch.sum(spk_out, 0)  # sum over time\n",
        "          _, neuron_max_act_total_out = torch.max(act_total_out, 1)  # argmax over output units to compare to labels\n",
        "          batch_acc.append(np.mean((neuron_max_act_total_out == labels).detach().cpu().numpy()))\n",
        "\n",
        "      if label_probabilities:\n",
        "          log_softmax_fn = nn.LogSoftmax(dim=-1)\n",
        "          log_p_y = log_softmax_fn(act_total_out)\n",
        "          if return_spikes:\n",
        "            return [np.mean(batch_loss), np.mean(batch_acc)], torch.exp(log_p_y), spk_out.detach().cpu().numpy()\n",
        "          else:\n",
        "            return [np.mean(batch_loss), np.mean(batch_acc)], torch.exp(log_p_y)\n",
        "      else:\n",
        "        if return_spikes:\n",
        "          return [np.mean(batch_loss), np.mean(batch_acc)], spk_out.detach().cpu().numpy()\n",
        "        else:\n",
        "          return [np.mean(batch_loss), np.mean(batch_acc)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83ycdkbwEHg8"
      },
      "source": [
        "### General settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fuQay3IDEPMl"
      },
      "outputs": [],
      "source": [
        "use_seed = True\n",
        "\n",
        "if use_seed:\n",
        "    seed = 42\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "else:\n",
        "    seed = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RurYq3JhEXFV"
      },
      "outputs": [],
      "source": [
        "device = \"cuda:0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQga4Aa7HaM1"
      },
      "source": [
        "## Import HAR dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP4DJ4wYCXx6"
      },
      "source": [
        "NOTE that we won't normalize data in this case, differently from what was done for the psMNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pZK3qVd6erR"
      },
      "outputs": [],
      "source": [
        "### Link to the folder with data\n",
        "folder_link = \"https://drive.google.com/drive/folders/15TSYpE5QSzjOoqvOn8f7Wf59MwhqQEkB\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0l0NUOX6XjC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if \"Data\" not in os.listdir(\"./\"):\n",
        "  ! gdown $folder_link -O ./Data --folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7McG954n64Ii"
      },
      "outputs": [],
      "source": [
        "ds_train = torch.load(\"./Data/watch_subset2_40_ds_train.pt\", map_location=device)\n",
        "ds_val = torch.load(\"./Data/watch_subset2_40_ds_val.pt\", map_location=device)\n",
        "ds_test = torch.load(\"./Data/watch_subset2_40_ds_test.pt\", map_location=device)\n",
        "\n",
        "act_map = {\n",
        "    'A': 'walking',\n",
        "    'B': 'jogging',\n",
        "    'C': 'stairs',\n",
        "    'D': 'sitting',\n",
        "    'E': 'standing',\n",
        "    'M': 'kicking',\n",
        "    'P': 'dribbling',\n",
        "    'O': 'catch',\n",
        "    'F': 'typing',\n",
        "    'Q': 'writing',\n",
        "    'R': 'clapping',\n",
        "    'G': 'teeth',\n",
        "    'S': 'folding',\n",
        "    'J': 'pasta',\n",
        "    'H': 'soup',\n",
        "    'L': 'sandwich',\n",
        "    'I': 'chips',\n",
        "    'K': 'drinking',\n",
        "}\n",
        "\n",
        "labels_mapping = {k:act_map[k] for k in list(act_map.keys())[6:13] if k in act_map}\n",
        "labels_activity = list(act_map.values())[6:13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft_EYrM08PwK"
      },
      "outputs": [],
      "source": [
        "# Extract a random sample from the training set\n",
        "random_sample = next(iter(DataLoader(ds_train, batch_size=1, shuffle=False)))\n",
        "random_data = random_sample[0]\n",
        "random_label = random_sample[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuRIrdBU9Uci"
      },
      "outputs": [],
      "source": [
        "# Randomly select a channel from IMU data\n",
        "rnd_ch = np.random.randint(0,6)\n",
        "print(\"Selected sample: {}\".format(labels_activity[random_label]))\n",
        "print(\"Selected channel: {}\\n\".format(rnd_ch))\n",
        "\n",
        "random_ch = random_data[0,:,rnd_ch]\n",
        "print(\"IMU values in time for the selected sample and channel:\")\n",
        "random_ch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq8kEA4fI312"
      },
      "outputs": [],
      "source": [
        "# Plot the random (original) example\n",
        "plt.figure(figsize=(8,4.5))\n",
        "plt.plot(random_ch.cpu().numpy())\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.ylabel(\"IMU data (a.u.)\")\n",
        "plt.title(\"Activity: {}\".format(labels_activity[random_label]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQOTbR0zJ7N2"
      },
      "outputs": [],
      "source": [
        "print(\"Training set \\t ---data---\\n \\tnumber of samples: {}\\n \\tsample shape: {}\".format(\n",
        "    len(ds_train),next(iter(DataLoader(ds_train, batch_size=1, shuffle=False)))[0].shape))\n",
        "print(\"Training set \\t ---labels---\\n \\tnumber of labels: {}\\n \\tlabel shape: {}\".format(\n",
        "    len(ds_train),next(iter(DataLoader(ds_train, batch_size=1, shuffle=False)))[1].shape))\n",
        "print(\"\\n\")\n",
        "print(\"Validation set \\t ---data---\\n \\tnumber of samples: {}\\n \\tshape: {}\".format(\n",
        "    len(ds_val),next(iter(DataLoader(ds_val, batch_size=1, shuffle=False)))[0].shape))\n",
        "print(\"Validation set \\t ---labels---\\n \\tnumber of labels: {}\\n \\tlabel shape: {}\".format(\n",
        "    len(ds_val),next(iter(DataLoader(ds_val, batch_size=1, shuffle=False)))[1].shape))\n",
        "print(\"\\n\")\n",
        "print(\"Test set \\t ---data---\\n \\tnumber of samples: {}\\n \\tshape: {}\".format(\n",
        "    len(ds_test),next(iter(DataLoader(ds_test, batch_size=1, shuffle=False)))[0].shape))\n",
        "print(\"Test set \\t ---labels---\\n \\tnumber of labels: {}\\n \\tlabel shape: {}\".format(\n",
        "    len(ds_test),next(iter(DataLoader(ds_test, batch_size=1, shuffle=False)))[1].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4peZee6B6Uc"
      },
      "source": [
        "## snnTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UxA8MFmE569"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "from snntorch import functional as SF\n",
        "from snntorch import utils\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "network_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZISpcrO3JUGv"
      },
      "source": [
        "### Feedforward SNN\n",
        "\n",
        "Training, Validation and Test of a FFSNN fully-connected\n",
        "\n",
        "*Adapted from: V. Fra et al.; \"Neuromorphic Human Activity Recognition through LIF-based neurons\"; Brain-Inspired Computing Workshop 2023, Modena (Italy)*\n",
        "\n",
        "\n",
        "Neurobench Metrics extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrnyIv-LWg0W"
      },
      "outputs": [],
      "source": [
        "settings= {\"enc_pop\": 32,\n",
        "           \"nb_hidden\": 250,\n",
        "           \"beta_hid\": 0.7,\n",
        "           \"beta_out\": 0.65,\n",
        "           \"beta_enc\": 0.2,\n",
        "           \"thr_enc\": 0.5,\n",
        "           \"thr_hid\": 0.7,\n",
        "           \"thr_out\": 0.9,\n",
        "           \"lr\": 0.0001,\n",
        "           \"batch_size\": 64\n",
        "           }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3kuATbQJWOv"
      },
      "outputs": [],
      "source": [
        "### Network structure (input data --> encoding -> hidden -> output)\n",
        "input_enc = 6 ##### THE FIRST SIZE OF THE INPUT LAYER IS EQUAL TO THE NUMBER OF CHANNELS OF EACH SAMPLE IN THE DATASET\n",
        "output_enc = int(settings[\"enc_pop\"])\n",
        "num_hidden = int(settings[\"nb_hidden\"])\n",
        "num_outputs = 7 ##### THE NUMBER OF OUTPUT NEURONS IS EQUAL TO THE NUMBER OF CLASSES IN THE DATSET\n",
        "\n",
        "num_steps = 40 ##### AS NUMBER OF TIME STEPS WE USE THE INPUT LENGTH\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    ##### Initialize layers #####\n",
        "    ### Encoding layer\n",
        "    self.enc = nn.Linear(input_enc, output_enc)\n",
        "    self.s_enc = snn.Leaky(beta=settings['beta_enc'], threshold=settings['thr_enc'])\n",
        "    ### Hidden layer\n",
        "    self.fc1 = nn.Linear(output_enc, num_hidden)\n",
        "    self.s1 = snn.Synaptic(beta=settings['beta_hid'], alpha=settings['alpha_hid'], threshold=settings['thr_hid'])\n",
        "    ### Output layer\n",
        "    self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
        "    self.s2 = snn.Synaptic(beta=settings['beta_out'], alpha=settings['alpha_out'], threshold=settings['thr_out'])\n",
        "    self.mem_enc = self.s_enc.init_leaky()\n",
        "    ### Hidden layer\n",
        "    self.syn1, self.mem1 = self.s1.init_synaptic()\n",
        "    ### Output layer\n",
        "    self.syn2, self.mem2 = self.s2.init_synaptic()\n",
        "\n",
        "  def single_forward(self, x):\n",
        "\n",
        "    ### Encoding layer\n",
        "    mem_enc = self.s_enc.init_leaky()\n",
        "    ### Hidden layer\n",
        "    syn1, mem1 = self.s1.init_synaptic()\n",
        "    ### Output layer\n",
        "    syn2, mem2 = self.s2.init_synaptic()\n",
        "\n",
        "    # Record the final layer\n",
        "    spk2_rec = []\n",
        "    syn2_rec = []\n",
        "    mem2_rec = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "      ### Encoding layer\n",
        "      cur_enc = self.enc(x[:,step])\n",
        "      spk_enc, mem_enc = self.s_enc(cur_enc, mem_enc)\n",
        "      ### Hidden layer\n",
        "      cur1 = self.fc1(spk_enc)\n",
        "      spk1, syn1, mem1 = self.s1(cur1, syn1, mem1)\n",
        "      ### Output layer\n",
        "      cur2 = self.fc2(spk1)\n",
        "      spk2, syn2, mem2 = self.s2(cur2, syn2, mem2)\n",
        "\n",
        "      spk2_rec.append(spk2)\n",
        "      syn2_rec.append(syn2)\n",
        "      mem2_rec.append(mem2)\n",
        "\n",
        "    return torch.stack(spk2_rec, dim=0), torch.stack(syn2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "  def forward(self, x):\n",
        "       ### Encoding layer\n",
        "      cur_enc = self.enc(x)\n",
        "      spk_enc, self.mem_enc = self.s_enc(cur_enc, self.mem_enc)\n",
        "      ### Hidden layer\n",
        "      cur1 = self.fc1(spk_enc)\n",
        "      spk1, self.syn1, self.mem1 = self.s1(cur1, self.syn1, self.mem1)\n",
        "      ### Output layer\n",
        "      cur2 = self.fc2(spk1)\n",
        "      spk2, self.syn2, self.mem2 = self.s2(cur2, self.syn2, self.mem2)\n",
        "\n",
        "      return spk2, self.mem2\n",
        "\n",
        "  def reset(self):\n",
        "    self.mem_enc = self.s_enc.init_leaky()\n",
        "    ### Hidden layer\n",
        "    self.syn1, self.mem1 = self.s1.init_synaptic()\n",
        "    ### Output layer\n",
        "    self.syn2, self.mem2 = self.s2.init_synaptic()\n",
        "\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STet-LLUN6ov"
      },
      "outputs": [],
      "source": [
        "### Set the loss function\n",
        "loss_fn = SF.ce_count_loss()\n",
        "\n",
        "### Set the optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=settings['lr'], betas=(0.9, 0.999))\n",
        "\n",
        "### Set the batch size\n",
        "batch_size = settings[\"batch_size\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPfOJW8bPV1A"
      },
      "source": [
        "#### Training (with validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xst_Q5xPLjg"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "training_results = []\n",
        "validation_results = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  train_loss, train_acc = training_loop(ds_train, batch_size, net, optimizer, loss_fn, device)\n",
        "  val_loss, val_acc = val_test_loop(ds_val, batch_size, net, loss_fn, device)\n",
        "\n",
        "  training_results.append([train_loss, train_acc])\n",
        "  validation_results.append([val_loss, val_acc])\n",
        "\n",
        "  print(\"Epoch {}/{}: \\n\\ttraining loss: {} \\n\\tvalidation loss: {} \\n\\ttraining accuracy: {}% \\n\\tvalidation accuracy: {}%\".format(epoch+1, num_epochs, training_results[-1][0], validation_results[-1][0], np.round(training_results[-1][1]*100,4), np.round(validation_results[-1][1]*100,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKpsa5FUPaDX"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DQhZw0YPbZY"
      },
      "outputs": [],
      "source": [
        "test_results, lbl_probs, spk_out = val_test_loop(ds_test, batch_size, net, loss_fn, device, label_probabilities=True, return_spikes=True)\n",
        "\n",
        "print(\"\\nTest accuracy: {}%\".format(np.round(test_results[1]*100,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUEEnask2qnk"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvWjUi8j2ev3"
      },
      "outputs": [],
      "source": [
        "create_directory('neurobench/model_data')\n",
        "torch.save(net.state_dict(), 'neurobench/model_data/HAR_FFSNN.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGVwG0kmHBn9"
      },
      "source": [
        "#### Neurobench Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJqwmLeFG_Po"
      },
      "outputs": [],
      "source": [
        "net = Net().to('cuda:0')\n",
        "net.load_state_dict(torch.load('neurobench/model_data/HAR_FFSNN.pth'))\n",
        "\n",
        "model = SNNTorchModel(net)\n",
        "test_set_loader = DataLoader(ds_test, batch_size=settings[\"batch_size\"], shuffle=False, drop_last=True)\n",
        "postprocessors = [choose_max_count]\n",
        "\n",
        "static_metrics = [\"model_size\"]\n",
        "data_metrics = [\"classification_accuracy\",  \"multiply_accumulates\"]\n",
        "\n",
        "benchmark = Benchmark(model, test_set_loader, [], postprocessors, [static_metrics, data_metrics])\n",
        "results = benchmark.run()\n",
        "\n",
        "results = [results[key] for key in results.keys()]\n",
        "results.insert(0, 'FFSNN')\n",
        "\n",
        "network_results.append(copy.copy(results))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ztuHhcAwzz0"
      },
      "source": [
        "#### Single-sample inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urOgz_rIsyew"
      },
      "outputs": [],
      "source": [
        "single_sample = next(iter(DataLoader(ds_test, batch_size=1, shuffle=True)))\n",
        "print(\"Randomly selected sample: {}\".format(labels_activity[single_sample[1].cpu()[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZcLZrrmuple"
      },
      "outputs": [],
      "source": [
        "### Inference\n",
        "_, lbl_probs, spk_out = val_test_loop(TensorDataset(single_sample[0],single_sample[1]), 1, net, loss_fn, device, label_probabilities=True, return_spikes=True)\n",
        "\n",
        "### Plot output spiking activity\n",
        "spk_out = np.moveaxis(spk_out,1,2)\n",
        "spk_out = np.squeeze(spk_out, axis=-1)\n",
        "spk_out.shape\n",
        "aer = []\n",
        "for num,el in enumerate(spk_out):\n",
        "  addr = np.where(el)[0].tolist()\n",
        "  if len(addr) > 0:\n",
        "    for ii in addr:\n",
        "      aer.append([num,ii])\n",
        "aer = np.array(aer)\n",
        "plt.scatter(aer[:,0], aer[:,1], s=1)\n",
        "plt.xlabel(\"Time step (a.u.)\")\n",
        "plt.ylabel(\"Neuron\")\n",
        "plt.title(\"Spiking output activity (activity: {}, prediction: {})\".format(labels_activity[single_sample[1].cpu()[0]],labels_activity[np.argmax(lbl_probs.cpu())]))\n",
        "plt.ylim(-0.5,6.5)\n",
        "plt.xlim((-0.5,num_steps+0.5))\n",
        "plt.yticks(range(7),labels_activity)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLabels probabilities:\")\n",
        "for num,el in enumerate(labels_activity):\n",
        "  print(\"\\t{} \\n\\t\\t{}%\".format(el,np.round(lbl_probs.cpu().numpy()[0][num]*100,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recurrent SNN\n",
        "\n",
        "Training, Validation and Test of a fully-connected RSNN\n",
        "\n",
        "*Adapted from: V. Fra et al.; \"Neuromorphic Human Activity Recognition through LIF-based neurons\"; Brain-Inspired Computing Workshop 2023, Modena (Italy)*\n",
        "\n",
        "\n",
        "Neurobench Metrics extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "settings = {\"neurons_per_pop\": 5.0, \n",
        "            \"output_pop\": 32.0, \n",
        "            \"nb_hidden\": 250.0, \n",
        "            \"alpha_hid\": 0.55, \n",
        "            \"alpha_out\": 0.9, \n",
        "            \"beta_hid\": 0.7, \n",
        "            \"beta_out\": 0.65, \n",
        "            \"beta_enc\": 0.2, \n",
        "            \"lr\": 0.0001, \n",
        "            \"slope\": 15.0, \n",
        "            \"batch_size\": 64.0\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Network structure (input data --> encoding -> hidden -> output)\n",
        "input_channels = 6\n",
        "pop_size = int(settings[\"neurons_per_pop\"]) # --> the number of neurons for the encoding layer with populations will be: pop_size*input_channels\n",
        "output_pop = int(settings[\"output_pop\"])\n",
        "output_enc = output_pop*input_channels \n",
        "num_hidden = int(settings[\"nb_hidden\"])\n",
        "num_outputs = 7\n",
        "\n",
        "### Surrogate gradient setting\n",
        "spike_grad = surrogate.fast_sigmoid(slope=int(settings[\"slope\"]))\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        ##### Define layers #####\n",
        "        ### Encoding layer with populations\n",
        "        self.pop_size = pop_size\n",
        "        self.enc_pops = []\n",
        "        self.lif_enc_pops = []\n",
        "        for ii in range(input_channels):\n",
        "            self.enc_pops.append(nn.Linear(pop_size, output_pop).to(device))\n",
        "            self.lif_enc_pops.append(snn.Leaky(beta=settings[\"beta_enc\"], learn_beta=True, learn_threshold=True).to(device))\n",
        "        self.enc_pops = nn.ModuleList(self.enc_pops)\n",
        "        self.lif_enc_pops = nn.ModuleList(self.lif_enc_pops)\n",
        "        ### Recurrent layer\n",
        "        self.fc1 = nn.Linear(output_enc, num_hidden)\n",
        "        self.lif1 = snn.RSynaptic(alpha=settings[\"alpha_hid\"], beta=settings[\"beta_hid\"], learn_alpha=True, learn_beta=True, learn_threshold=True, linear_features=num_hidden, spike_grad=spike_grad)\n",
        "        ### Output layer\n",
        "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
        "        self.lif2 = snn.Synaptic(alpha=settings[\"alpha_out\"], beta=settings[\"beta_out\"], learn_alpha=True, learn_beta=True, learn_threshold=True)\n",
        "    \n",
        "    \n",
        "    def single_forward(self, x):\n",
        "        ##### Initialize hidden states at t=0 #####\n",
        "        ### Encoding layer with populations\n",
        "        mem_pops_enc = torch.empty((input_channels,x.shape[1],output_pop), dtype=torch.float, device=device)\n",
        "        spk_pops_enc = torch.empty((input_channels,x.shape[1],output_pop), dtype=torch.float, device=device)\n",
        "        cur_pops_enc = torch.empty((input_channels,x.shape[1],output_pop), dtype=torch.float, device=device)\n",
        "        ### Recurrent layer\n",
        "        spk1, syn1, mem1 = self.lif1.init_rsynaptic()\n",
        "        ### Output layer\n",
        "        syn2, mem2 = self.lif2.init_synaptic()\n",
        "        \n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        syn2_rec = []\n",
        "        mem2_rec = []\n",
        "        for step in range(num_steps):\n",
        "            ### Encoding layer with populations\n",
        "            for num,el in enumerate(self.enc_pops):\n",
        "                cur_pops_enc[num] = el(torch.tile(x[step,:,num],(self.pop_size,1)).swapaxes(1,0))\n",
        "            for num,el in enumerate(self.lif_enc_pops):\n",
        "                spk_pops_enc[num], mem_pops_enc[num] = el(cur_pops_enc[num], mem_pops_enc[num])\n",
        "            spk_enc = spk_pops_enc.clone().permute(1, 0, 2).reshape((x.shape[1],input_channels*output_pop)).requires_grad_(True)\n",
        "            ### Recurrent layer\n",
        "            cur1 = self.fc1(spk_enc) # self.fc1(x[step])\n",
        "            spk1, syn1, mem1 = self.lif1(cur1, spk1, syn1, mem1)\n",
        "            ### Output layer\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, syn2, mem2 = self.lif2(cur2, syn2, mem2)\n",
        "            \n",
        "            spk2_rec.append(spk2)\n",
        "            syn2_rec.append(syn2)\n",
        "            mem2_rec.append(mem2)\n",
        "        \n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(syn2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        ### Encoding layer\n",
        "        cur_enc = self.enc(x)\n",
        "        spk_enc, self.mem_enc = self.s_enc(cur_enc, self.mem_enc)\n",
        "\n",
        "        ### Recurrent layer\n",
        "        cur1 = self.fc1(spk_enc)\n",
        "        spk1, self.syn1, self.mem1 = self.s1(cur1, self.syn1, self.mem1)\n",
        "\n",
        "        ### Output layer\n",
        "        cur2 = self.fc2(spk1)\n",
        "        spk2, self.syn2, self.mem2 = self.s2(cur2, self.syn2, self.mem2)\n",
        "\n",
        "        return spk2, self.mem2\n",
        "\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Set the loss function\n",
        "loss_fn = SF.ce_count_loss()\n",
        "\n",
        "### Set the optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=settings['lr'], betas=(0.9, 0.999))\n",
        "\n",
        "### Set the batch size\n",
        "batch_size = settings[\"batch_size\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training (with validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "training_results = []\n",
        "validation_results = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  train_loss, train_acc = training_loop(ds_train, batch_size, net, optimizer, loss_fn, device)\n",
        "  val_loss, val_acc = val_test_loop(ds_val, batch_size, net, loss_fn, device)\n",
        "\n",
        "  training_results.append([train_loss, train_acc])\n",
        "  validation_results.append([val_loss, val_acc])\n",
        "\n",
        "  print(\"Epoch {}/{}: \\n\\ttraining loss: {} \\n\\tvalidation loss: {} \\n\\ttraining accuracy: {}% \\n\\tvalidation accuracy: {}%\".format(epoch+1, num_epochs, training_results[-1][0], validation_results[-1][0], np.round(training_results[-1][1]*100,4), np.round(validation_results[-1][1]*100,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results, lbl_probs, spk_out = val_test_loop(ds_test, batch_size, net, loss_fn, device, label_probabilities=True, return_spikes=True)\n",
        "\n",
        "print(\"\\nTest accuracy: {}%\".format(np.round(test_results[1]*100,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_directory('neurobench/model_data')\n",
        "torch.save(net.state_dict(), 'neurobench/model_data/HAR_RSNN.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Neurobench Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = Net().to('cuda:0')\n",
        "net.load_state_dict(torch.load('neurobench/model_data/HAR_RSNN.pth'))\n",
        "\n",
        "model = SNNTorchModel(net)\n",
        "test_set_loader = DataLoader(ds_test, batch_size=settings[\"batch_size\"], shuffle=False, drop_last=True)\n",
        "postprocessors = [choose_max_count]\n",
        "\n",
        "static_metrics = [\"model_size\"]\n",
        "data_metrics = [\"classification_accuracy\",  \"multiply_accumulates\"]\n",
        "\n",
        "benchmark = Benchmark(model, test_set_loader, [], postprocessors, [static_metrics, data_metrics])\n",
        "results = benchmark.run()\n",
        "\n",
        "results = [results[key] for key in results.keys()]\n",
        "results.insert(0, 'RSNN')\n",
        "\n",
        "network_results.append(copy.copy(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Single-sample inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "single_sample = next(iter(DataLoader(ds_test, batch_size=1, shuffle=True)))\n",
        "print(\"Randomly selected sample: {}\".format(labels_activity[single_sample[1].cpu()[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Inference\n",
        "_, lbl_probs, spk_out = val_test_loop(TensorDataset(single_sample[0],single_sample[1]), 1, net, loss_fn, device, label_probabilities=True, return_spikes=True)\n",
        "\n",
        "### Plot output spiking activity\n",
        "spk_out = np.moveaxis(spk_out,1,2)\n",
        "spk_out = np.squeeze(spk_out, axis=-1)\n",
        "spk_out.shape\n",
        "aer = []\n",
        "for num,el in enumerate(spk_out):\n",
        "  addr = np.where(el)[0].tolist()\n",
        "  if len(addr) > 0:\n",
        "    for ii in addr:\n",
        "      aer.append([num,ii])\n",
        "aer = np.array(aer)\n",
        "plt.scatter(aer[:,0], aer[:,1], s=1)\n",
        "plt.xlabel(\"Time step (a.u.)\")\n",
        "plt.ylabel(\"Neuron\")\n",
        "plt.title(\"Spiking output activity (activity: {}, prediction: {})\".format(labels_activity[single_sample[1].cpu()[0]],labels_activity[np.argmax(lbl_probs.cpu())]))\n",
        "plt.ylim(-0.5,6.5)\n",
        "plt.xlim((-0.5,num_steps+0.5))\n",
        "plt.yticks(range(7),labels_activity)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLabels probabilities:\")\n",
        "for num,el in enumerate(labels_activity):\n",
        "  print(\"\\t{} \\n\\t\\t{}%\".format(el,np.round(lbl_probs.cpu().numpy()[0][num]*100,2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlYI1jKB01PY"
      },
      "source": [
        "### Convolutional SNN\n",
        "\n",
        "Training, Validation and Test of a CSNN\n",
        "\n",
        "Neurobench Metrics extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erDlQe8HRlr-"
      },
      "outputs": [],
      "source": [
        "settings = {\n",
        "    'conv_1_out_fts': 200,\n",
        "    'conv_1_kernel_size': 2,\n",
        "    'maxpool_1_fts': 2,\n",
        "    'conv_1_pad': 4,\n",
        "    'leaky_1_beta': 0.4,\n",
        "    'leaky_1_thr': 0.002,\n",
        "    'conv_2_in_fts': 100,\n",
        "    'conv_2_out_fts': 256,\n",
        "    'conv_2_kernel_size': 1,\n",
        "    'maxpool_2_fts': 2,\n",
        "    'leaky_2_beta': 0.3,\n",
        "    'leaky_2_thr': 0.001,\n",
        "    'leaky_3_beta': 0.5,\n",
        "    'leaky_3_thr': 0.001,\n",
        "    'lr': 1.e-3,\n",
        "    'batch_size': 256,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z4AsVtN4Yo7"
      },
      "outputs": [],
      "source": [
        "input_enc = 6 ##### THE FIRST SIZE OF THE INPUT LAYER IS EQUAL TO THE NUMBER OF CHANNELS OF EACH SAMPLE IN THE DATASET\n",
        "num_outputs = 7 ##### THE NUMBER OF OUTPUT NEURONS IS EQUAL TO THE NUMBER OF CLASSES IN THE DATSET\n",
        "num_steps = 40 ##### AS NUMBER OF TIME STEPS WE USE THE INPUT LENGTH\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(input_enc, settings['conv_1_out_fts'], kernel_size=settings['conv_1_kernel_size'], padding=settings['conv_1_pad'])\n",
        "        self.max1 = nn.MaxPool2d(settings['maxpool_1_fts'])\n",
        "        self.leaky1 = snn.Leaky(beta=settings['leaky_1_beta'], init_hidden=True, threshold=settings['leaky_1_thr'])\n",
        "        self.conv2 = nn.Conv1d(settings['conv_2_in_fts'], settings['conv_2_out_fts'], kernel_size=settings['conv_2_kernel_size'])\n",
        "        self.max2 = nn.MaxPool2d(settings['maxpool_2_fts'])\n",
        "        self.leaky2 = snn.Leaky(beta=settings['leaky_2_beta'], init_hidden=True, threshold=settings['leaky_2_thr'])\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(settings['conv_2_out_fts'], num_outputs)\n",
        "        self.leaky3 = snn.Leaky(beta=settings['leaky_3_beta'], output=True, init_hidden=True, threshold=settings['leaky_3_thr'])\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        x = self.conv1(input.reshape(input.shape[0], input.shape[1], 1))\n",
        "        x = self.max1(x)\n",
        "        x = self.leaky1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.max2(x)\n",
        "        x = self.leaky2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        spk_out, mem_out = self.leaky3(x)\n",
        "        return spk_out, mem_out\n",
        "\n",
        "    def single_forward(self, input):\n",
        "        mem_rec = []\n",
        "        spk_rec = []\n",
        "\n",
        "        self.leaky1.init_leaky()\n",
        "        self.leaky2.init_leaky()\n",
        "        self.leaky3.init_leaky()\n",
        "        utils.reset(self)\n",
        "        for step in range(num_steps):\n",
        "\n",
        "            new_input = input[:, step, :]\n",
        "            x = self.conv1(new_input.reshape(new_input.shape[0], new_input.shape[1], 1))\n",
        "            x = self.max1(x)\n",
        "            x = self.leaky1(x)\n",
        "            x = self.conv2(x)\n",
        "            x = self.max2(x)\n",
        "            x = self.leaky2(x)\n",
        "            x = self.flatten(x)\n",
        "            x = self.linear(x)\n",
        "            spk_out, mem_out = self.leaky3(x)\n",
        "\n",
        "            spk_rec.append(spk_out)\n",
        "            mem_rec.append(mem_out)\n",
        "\n",
        "        return torch.stack(spk_rec), torch.stack(mem_rec)\n",
        "\n",
        "    def reset(self):\n",
        "      self.leaky1.init_leaky()\n",
        "      self.leaky2.init_leaky()\n",
        "      self.leaky3.init_leaky()\n",
        "\n",
        "    def frequency(self):\n",
        "      acquisition_rate = 20\n",
        "      temporal_window = num_steps/acquisition_rate\n",
        "      return 1/temporal_window\n",
        "\n",
        "\n",
        "net = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uksy-_-5zKc"
      },
      "outputs": [],
      "source": [
        "### Set the loss function\n",
        "loss_fn = SF.ce_count_loss()\n",
        "\n",
        "### Set the optimizer\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=settings['lr'], betas=(0.9, 0.999))\n",
        "\n",
        "### Set the batch size\n",
        "batch_size = settings[\"batch_size\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzyok0ia567Y"
      },
      "source": [
        "#### Training (with validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE9Q9FEh6EyC"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "\n",
        "training_results = []\n",
        "validation_results = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  train_loss, train_acc = training_loop(ds_train, batch_size, net, optimizer, loss_fn, device)\n",
        "  val_loss, val_acc = val_test_loop(ds_val, batch_size, net, loss_fn, device)\n",
        "\n",
        "  training_results.append([train_loss, train_acc])\n",
        "  validation_results.append([val_loss, val_acc])\n",
        "\n",
        "  print(\"Epoch {}/{}: \\n\\ttraining loss: {} \\n\\tvalidation loss: {} \\n\\ttraining accuracy: {}% \\n\\tvalidation accuracy: {}%\".format(epoch+1, num_epochs, training_results[-1][0], validation_results[-1][0], np.round(training_results[-1][1]*100,4), np.round(validation_results[-1][1]*100,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mp5OlDs6Nn7"
      },
      "source": [
        "#### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa__FLVs6NoX"
      },
      "outputs": [],
      "source": [
        "test_results, lbl_probs, spk_out = val_test_loop(ds_test, batch_size, net, loss_fn, device, label_probabilities=True, return_spikes=True)\n",
        "\n",
        "print(\"\\nTest accuracy: {}%\".format(np.round(test_results[1]*100,4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQDFquzA6Wdp"
      },
      "source": [
        "#### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja0VpKMh6Wdq"
      },
      "outputs": [],
      "source": [
        "create_directory('neurobench/model_data')\n",
        "torch.save(net.state_dict(), 'neurobench/model_data/HAR_SCNN.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AndMz7gO6ea3"
      },
      "source": [
        "#### Neurobench Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyu3eJoz6ea4"
      },
      "outputs": [],
      "source": [
        "net = Net().to('cuda:0')\n",
        "net.load_state_dict(torch.load('neurobench/model_data/HAR_SCNN.pth'))\n",
        "\n",
        "model = SNNTorchModel(net)\n",
        "test_set_loader = DataLoader(ds_test, batch_size=settings[\"batch_size\"], shuffle=False, drop_last=True)\n",
        "postprocessors = [choose_max_count]\n",
        "\n",
        "static_metrics = [\"model_size\"]\n",
        "data_metrics = [\"classification_accuracy\",  \"multiply_accumulates\"]\n",
        "\n",
        "benchmark = Benchmark(model, test_set_loader, [], postprocessors, [static_metrics, data_metrics])\n",
        "results = benchmark.run()\n",
        "\n",
        "results = [results[key] for key in results.keys()]\n",
        "results.insert(0, 'SCNN')\n",
        "\n",
        "network_results.append(copy.copy(results))\n",
        "\n",
        "\n",
        "#print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiYB2dKrBol2"
      },
      "source": [
        "### Networks Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y51YN_a9ASgx"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(network_results, columns=[\"Model Name\", \"Model Size\", \"Classification Accuracy\", \"Multiply Accumulates\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cc4eYiEoCchd",
        "V0pq1QdFDEH9",
        "83ycdkbwEHg8",
        "qQga4Aa7HaM1",
        "2ztuHhcAwzz0",
        "dzyok0ia567Y",
        "AndMz7gO6ea3",
        "SiYB2dKrBol2"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "2e5f42e4452efdb55deabd82d647dd2921cc3aaae1fb4d0764999ca11e054984"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
