{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/home/shenqi/Master_thesis')\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from Neuronbench.utils.models import Vanilla\n",
    "from Neuronbench.utils.dataset import seq_dataloader\n",
    "\n",
    "from metavision_ml.detection.anchors import Anchors\n",
    "from metavision_ml.detection.rpn import BoxHead\n",
    "from metavision_ml.data import box_processing as box_api\n",
    "from metavision_ml.detection.losses import DetectionLoss\n",
    "from metavision_ml.metrics.coco_eval import CocoEvaluator\n",
    "from metavision_sdk_core import EventBbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = seq_dataloader(num_tbins=1, batch_size = 1)\n",
    "model = Vanilla(cin = dataloader.in_channels, cout = 256, base = 16)\n",
    "box_coder = Anchors(num_levels=model.levels, anchor_list=\"PSEE_ANCHORS\", variances=[0.1, 0.2])\n",
    "head = BoxHead(model.cout, box_coder.num_anchors, len(dataloader.wanted_keys) + 1, 0)\n",
    "model = model.to('cuda')\n",
    "head = head.to('cuda')\n",
    "model.load_state_dict(torch.load('../train_RED/save_models/25_model.pth',map_location=torch.device('cuda')))\n",
    "head.load_state_dict(torch.load('../train_RED/save_models/25_pd.pth',map_location=torch.device('cuda')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_predictions(preds, targets, video_infos, frame_is_labeled):\n",
    "       \n",
    "        dt_detections = {}\n",
    "        gt_detections = {}\n",
    "        for t in range(len(targets)):\n",
    "            for i in range(len(targets[t])):\n",
    "                gt_boxes = targets[t][i]\n",
    "                pred = preds[t][i]\n",
    "\n",
    "                video_info, tbin_start, _ = video_infos[i]\n",
    "\n",
    "                if video_info.padding or frame_is_labeled[t, i] == False:\n",
    "                    continue\n",
    "\n",
    "                name = video_info.path\n",
    "                if name not in dt_detections:\n",
    "                    dt_detections[name] = [np.zeros((0), dtype=box_api.EventBbox)]\n",
    "                if name not in gt_detections:\n",
    "                    gt_detections[name] = [np.zeros((0), dtype=box_api.EventBbox)]\n",
    "                assert video_info.start_ts == 0\n",
    "                ts = tbin_start + t * video_info.delta_t\n",
    "\n",
    "                if isinstance(gt_boxes, torch.Tensor):\n",
    "                    gt_boxes = gt_boxes.cpu().numpy()\n",
    "                if gt_boxes.dtype == np.float32:\n",
    "                    gt_boxes = box_api.box_vectors_to_bboxes(gt_boxes[:, :4], gt_boxes[:, 4], ts=ts)\n",
    "\n",
    "                if pred['boxes'] is not None and len(pred['boxes']) > 0:\n",
    "                    boxes = pred['boxes'].cpu().data.numpy()\n",
    "                    labels = pred['labels'].cpu().data.numpy()\n",
    "                    scores = pred['scores'].cpu().data.numpy()\n",
    "                    dt_boxes = box_api.box_vectors_to_bboxes(boxes, labels, scores, ts=ts)\n",
    "                    dt_detections[name].append(dt_boxes)\n",
    "                else:\n",
    "                    dt_detections[name].append(np.zeros((0), dtype=EventBbox))\n",
    "\n",
    "                if len(gt_boxes):\n",
    "                    gt_boxes[\"t\"] = ts\n",
    "                    gt_detections[name].append(gt_boxes)\n",
    "                else:\n",
    "                    gt_detections[name].append(np.zeros((0), dtype=EventBbox))\n",
    "\n",
    "        return dt_detections, gt_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████████████████████████████████████████████████| 1200/1200 [02:08<00:00,  9.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from metavision_ml.detection_tracking.display_frame import draw_box_events\n",
    "from skvideo.io import FFmpegWriter\n",
    "import cv2\n",
    "\n",
    "viz_labels = partial(draw_box_events, label_map=['background'] + dataloader.wanted_keys, thickness = 2)\n",
    "video_writer = FFmpegWriter('vis.mp4', outputdict={\n",
    "                                            '-vcodec': 'libx264', '-crf': '20', '-preset': 'veryslow','-r': '20'})\n",
    "size_x = 2\n",
    "size_y = 1\n",
    "height_scaled = 360\n",
    "width_scaled = 640\n",
    "frame = np.zeros((size_y * height_scaled, width_scaled * size_x, 3), dtype=np.uint8)\n",
    "\n",
    "model.eval()\n",
    "head.eval()\n",
    "\n",
    "loader = dataloader.seq_dataloader_val\n",
    "with tqdm(total=len(loader), desc=f'Testing',ncols=120) as pbar:\n",
    "    for ind, data in enumerate(loader):\n",
    "        pbar.update(1)\n",
    "        inputs = data['inputs'].to(device='cuda')\n",
    "        with torch.no_grad():\n",
    "            batch = data['inputs']\n",
    "            im =batch[0][0].cpu().numpy()\n",
    "            img = loader.get_vis_func()(im)\n",
    "            img_RED = img.copy()\n",
    "            labels = data[\"labels\"][0][0]\n",
    "            img = viz_labels(img, labels)\n",
    "            \n",
    "            feature = model(inputs)\n",
    "            loc_preds_val, cls_preds_val = head(feature)\n",
    "            scores = head.get_scores(cls_preds_val)\n",
    "            scores = scores.to('cpu')\n",
    "            for i, feat in enumerate(feature):\n",
    "                feature[i] = feature[i].to('cpu')\n",
    "            inputs = data['inputs'].to('cpu')\n",
    "            loc_preds_val = loc_preds_val.to('cpu')\n",
    "            preds = box_coder.decode(feature, inputs, loc_preds_val, scores, batch_size=inputs.shape[1], score_thresh=0.5,\n",
    "                        nms_thresh=0.5, max_boxes_per_input=500)\n",
    "            # print(preds)\n",
    "            dt_dic, gt_dic = accumulate_predictions(preds, data[\"labels\"], data[\"video_infos\"], data[\"frame_is_labeled\"])\n",
    "            output_dt_RED = list(dt_dic.values())\n",
    "            \n",
    "            if(len(output_dt_RED) < 1):\n",
    "                img_RED = viz_labels(img_RED, [])\n",
    "            else:\n",
    "                img_RED = viz_labels(img_RED, output_dt_RED[0][1])\n",
    "                \n",
    "            metadata = loader.dataset.get_batch_metadata(ind)\n",
    "            name = metadata[0][0].path.split('/')[-1]\n",
    "            cv2.putText(img, name, (int(0.05 * (width_scaled)), int(0.94 * (height_scaled))),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1.2, (50, 240, 12))\n",
    "            cv2.putText(img_RED, 'RED', (int(0.05 * (width_scaled)), int(0.94 * (height_scaled))),\n",
    "                            cv2.FONT_HERSHEY_PLAIN, 1.2, (50, 240, 12))\n",
    "            \n",
    "            y, x = divmod(0, size_x)\n",
    "            frame[y * (height_scaled):(y + 1) * (height_scaled),\n",
    "                x * (width_scaled): (x + 1) * (width_scaled)] = img\n",
    "            frame[y * (height_scaled):(y + 1) * (height_scaled),\n",
    "                (x+1) * (width_scaled): (x + 2) * (width_scaled)] = img_RED\n",
    "            video_writer.writeFrame(frame)\n",
    "video_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch1_13_cu117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
