{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGm4fad3M-Sr"
   },
   "source": [
    "#  DVS Object Detection Benchmark Tutorial\n",
    "\n",
    "This tutorial aims to provide an insight on how the NeuroBench framework is organized and how you can use it to benchmark your own models!\n",
    "\n",
    "## About DVS Object Detection:\n",
    "Real-time object detection is a widely used computer vision task with applications in several domains, including robotics, autonomous driving, and surveillance. Its applications include event cameras for smart home and surveillance systems, drones that monitor and track objects of interest, and self-driving cars that detect obstacles to ensure safe operation. Efficient energy consumption and real-time performance are crucial in such scenarios, particularly when deployed on low-power or always-on edge devices.\n",
    "\n",
    "### Dataset:\n",
    "The object detection benchmark utilizes the Prophesee 1 Megapixel Automotive Detection Dataset. This dataset was recorded with a high-resolution event camera with a 110 degree field of view mounted on a car windshield. The car was driven in various areas under different daytime weather conditions over several months. The dataset was labeled using the video stream of an additional RGB camera in a semi-automated way, resulting in over 25 million bounding boxes for seven different object classes: pedestrian, two-wheeler, car, truck, bus, traffic sign, and traffic light. The labels are provided at a rate of 60Hz, and the recording of 14.65 hours is split into 11.19, 2.21, and 2.25 hours for training, validation, and testing, respectively. \n",
    "\n",
    "### Benchmark Task:\n",
    "The task of object detection in event-based spatio-temporal data involves identifying bounding boxes of objects belonging to multiple predetermined classes in an event stream. Training for this task is performed offline based on the data splits provided by the original dataset.\n",
    "\n",
    "Note: This benchmark relies on the [Prophesee Metavision software](https://docs.prophesee.ai/stable/index.html), which must be downloaded from Prophesee itself and is not included with the NeuroBench package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will import the relevant libraries. These include the dataset, model wrapper, and benchmark object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from neurobench.datasets import Gen4DetectionDataLoader\n",
    "from neurobench.models import NeuroBenchModel\n",
    "from neurobench.benchmarks import Benchmark\n",
    "\n",
    "from metavision_ml.detection.anchors import Anchors\n",
    "from metavision_ml.detection.rpn import BoxHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we use the baseline RED architecture ([https://arxiv.org/pdf/2009.13436.pdf](https://arxiv.org/pdf/2009.13436.pdf)), labelled as Vanilla, and a hybrid ANN-SNN conversion which replaces the recurrent convolutional layers with spiking neurons, labelled Vanilla_lif. The latter model is implemented using the SpikingJelly framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.obj_detection.obj_det_model import Vanilla, Vanilla_lif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we will load our desired dataset in a dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader itself takes about 7 minutes for loading, with model evaluation and score calculation is about 20 minutes on i9-12900KF, RTX3080\n",
    "test_set_dataloader = Gen4DetectionDataLoader(dataset_path=\"../../data/Gen 4 Multi channel\", # data in repo root dir\n",
    "        split=\"testing\",\n",
    "        batch_size = 12,\n",
    "        num_tbins = 12,\n",
    "        preprocess_function_name=\"multi_channel_timesurface\",\n",
    "        delta_t=50000,\n",
    "        channels=6,  # multichannel six channels\n",
    "        height=360,\n",
    "        width=640,\n",
    "        max_incr_per_pixel=5,\n",
    "        class_selection=[\"pedestrian\", \"two wheeler\", \"car\"],\n",
    "        num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the models we want to benchmark, we need a wrapper. The wrapper inherits from the NeuroBenchModel base class and defines the `__init__`, `__call__`, and `__net__` functions. Note that the `__call__` function evaluates the whole inference pipeline and returns final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline written and models trained by Shenqi Wang (wang69@imec.be) and Guangzhi Tang (guangzhi.tang@imec.nl) at imec.\n",
    "\n",
    "class ObjDetectionModel(NeuroBenchModel):\n",
    "    def __init__(self, net, box_coder, head):\n",
    "        self.net = net\n",
    "        self.box_coder = box_coder\n",
    "        self.head = head\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        self.net.eval()\n",
    "        inputs = batch.permute(1, 0, 2, 3, 4).to(device='cuda') # dataloader supplies batch,timestep,*; model expects timestep,batch,*\n",
    "        with torch.no_grad():\n",
    "            feature = self.net(inputs)\n",
    "            loc_preds_val, cls_preds_val = self.head(feature)\n",
    "            scores = self.head.get_scores(cls_preds_val)\n",
    "            scores = scores.to('cpu')\n",
    "            for i, feat in enumerate(feature):\n",
    "                feature[i] = feature[i].to('cpu')\n",
    "            inputs = inputs.to('cpu')\n",
    "            loc_preds_val = loc_preds_val.to('cpu')\n",
    "            preds = box_coder.decode(feature, inputs, loc_preds_val, scores, batch_size=inputs.shape[1], score_thresh=0.05,\n",
    "                        nms_thresh=0.5, max_boxes_per_input=500)\n",
    "        return preds\n",
    "\n",
    "    def __net__(self):\n",
    "        # returns only network, not box_coder and head\n",
    "        return self.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load our model. This example includes two possibilities, a hybrid model which uses artificial neurons and spiking neurons or a fully artificial neural network without spiking neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the ANN or Hybrid model\n",
    "mode = \"hybrid\" # \"ann\" or \"hybrid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "if mode == \"ann\":\n",
    "    # baseline ANN RED architecture\n",
    "    model = Vanilla(cin = 6, cout = 256, base = 16)\n",
    "    box_coder = Anchors(num_levels=model.levels, anchor_list=\"PSEE_ANCHORS\", variances=[0.1, 0.2])\n",
    "    head = BoxHead(model.cout, box_coder.num_anchors, 3+1, 0)\n",
    "    model = model.to('cuda')\n",
    "    head = head.to('cuda')\n",
    "    model.load_state_dict(torch.load('model_data/save_models/25_ann_model.pth', map_location=torch.device('cuda')))\n",
    "    head.load_state_dict(torch.load('model_data/save_models/25_ann_pd.pth', map_location=torch.device('cuda')))\n",
    "elif mode == \"hybrid\":\n",
    "    # hybrid SNN of above architecture\n",
    "    model = Vanilla_lif(cin = 6, cout = 256, base = 16)\n",
    "    box_coder = Anchors(num_levels=model.levels, anchor_list=\"PSEE_ANCHORS\", variances=[0.1, 0.2])\n",
    "    head = BoxHead(model.cout, box_coder.num_anchors, 3+1, 0)\n",
    "    model = model.to('cuda')\n",
    "    head = head.to('cuda')\n",
    "    model.load_state_dict(torch.load('model_data/save_models/14_hybrid_model.pth', map_location=torch.device('cuda')))\n",
    "    head.load_state_dict(torch.load('model_data/save_models/14_hybrid_pd.pth', map_location=torch.device('cuda')))\n",
    "else:\n",
    "    raise ValueError(\"mode must be ann or hybrid\")\n",
    "\n",
    "model = ObjDetectionModel(model, box_coder, head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pre- or post-processors are needed for this benchmark task setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessors = []\n",
    "postprocessors = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next specify the metrics which you want to calculate.\n",
    "\n",
    "Note that the Model Excecution Rate metric is not returned by the famework, but reported by the user. Execution rate, in Hz, of the model computation based on forward inference passes per second, measured in the time-stepped simulation timescale. For both the ANN and Hybrid models, since raw event data is processed in non-overlapping 50 ms windows, the execution rate is 20 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_metrics = [\"footprint\", \"connection_sparsity\"]\n",
    "workload_metrics = [\"activation_sparsity\", \"COCO_mAP\", \"synaptic_operations\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to run the benchmark! The run may take a while (~1hr), as the event processing, model inference, and metric calculation are intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = Benchmark(model, test_set_dataloader, preprocessors, postprocessors, [static_metrics, workload_metrics])\n",
    "results = benchmark.run()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results:\n",
    "\n",
    "Results - ANN\n",
    "{'footprint': 91314912, 'connection_sparsity': 0.0, 'activation_sparsity': 0.6339577418819095, 'COCO_mAP': 0.4286601323956029, 'synaptic_operations': {'Effective_MACs': 248423062860.16266, 'Effective_ACs': 0.0, 'Dense': 284070730752.0}}\n",
    "\n",
    "Results - Hybrid\n",
    "{'footprint': 12133872, 'connection_sparsity': 0.0, 'activation_sparsity': 0.6130047485397788, 'COCO_mAP': 0.27111120859281557, 'synaptic_operations': {'Effective_MACs': 37520084211.538666, 'Effective_ACs': 559864693.7093333, 'Dense': 98513107968.0}}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
