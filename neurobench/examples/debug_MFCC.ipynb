{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, TensorDataset\n",
    "import torchaudio\n",
    "\n",
    "from torch import nn, optim, distributions as dist\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "# from torch_mate.data.utils import IncrementalFewShot\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home3/p306982/Simulations/fscil/algorithms_benchmarks/\")\n",
    "\n",
    "from neurobench.datasets import MSWC\n",
    "from neurobench.preprocessing.speech2spikes import S2SProcessor\n",
    "from neurobench.datasets.IncrementalFewShot import IncrementalFewShot\n",
    "from neurobench.examples.model_data.M5 import M5\n",
    "\n",
    "from neurobench.benchmarks import Benchmark\n",
    "from cl_utils import *\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import snntorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3674842/642652727.py:3: DeprecationWarning: The module snntorch.backprop will be deprecated in  a future release. Writing out your own training loop will lead to substantially faster performance.\n",
      "  from snntorch import backprop\n"
     ]
    }
   ],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import backprop\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"//scratch/p306982/data/fscil/mswc/\"\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"training\")\n",
    "pre_train_loader = DataLoader(base_train_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "base_test_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00, -3.0518e-05,  ..., -3.0518e-05,\n",
       "           0.0000e+00,  0.0000e+00]]),\n",
       " 48,\n",
       " '//scratch/p306982/data/fscil/mswc/en/clips',\n",
       " 'add/common_voice_en_100260.opus')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00, -3.0518e-05,  ..., -3.0518e-05,\n",
       "           0.0000e+00,  0.0000e+00]]),\n",
       " 48,\n",
       " '//scratch/p306982/data/fscil/mswc/en/clips',\n",
       " 'add/common_voice_en_100260.opus')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'see': 11526,\n",
       "  'think': 9085,\n",
       "  'work': 7944,\n",
       "  'find': 6633,\n",
       "  'home': 5877,\n",
       "  'look': 5137,\n",
       "  'music': 4657,\n",
       "  'next': 4576,\n",
       "  'heart': 4317,\n",
       "  'play': 3850,\n",
       "  'read': 3597,\n",
       "  'show': 3574,\n",
       "  'game': 3569,\n",
       "  'set': 3291,\n",
       "  'open': 3042,\n",
       "  'sound': 2769,\n",
       "  'story': 2702,\n",
       "  'understand': 2561,\n",
       "  'talk': 2503,\n",
       "  'keep': 2466,\n",
       "  'call': 2430,\n",
       "  'stop': 2398,\n",
       "  'change': 2172,\n",
       "  'hear': 2161,\n",
       "  'run': 2139,\n",
       "  'start': 2132,\n",
       "  'feel': 2115,\n",
       "  'remember': 2079,\n",
       "  'speak': 1978,\n",
       "  'eat': 1944,\n",
       "  'present': 1923,\n",
       "  'center': 1900,\n",
       "  'phone': 1845,\n",
       "  'order': 1820,\n",
       "  'learn': 1818,\n",
       "  'close': 1809,\n",
       "  'ask': 1782,\n",
       "  'style': 1732,\n",
       "  'season': 1720,\n",
       "  'news': 1709,\n",
       "  'research': 1660,\n",
       "  'turn': 1659,\n",
       "  'write': 1592,\n",
       "  'drink': 1574,\n",
       "  'search': 1567,\n",
       "  'market': 1561,\n",
       "  'track': 1493,\n",
       "  'project': 1489,\n",
       "  'add': 1482,\n",
       "  'watch': 1474,\n",
       "  'forget': 1464,\n",
       "  'position': 1432,\n",
       "  'listen': 1406,\n",
       "  'sort': 1394,\n",
       "  'design': 1334,\n",
       "  'bank': 1291,\n",
       "  'sit': 1271,\n",
       "  'video': 1248,\n",
       "  'save': 1241,\n",
       "  'deal': 1233,\n",
       "  'lights': 1229,\n",
       "  'meet': 1225,\n",
       "  'walk': 1212,\n",
       "  'cut': 1183,\n",
       "  'question': 1167,\n",
       "  'weather': 1158,\n",
       "  'date': 1131,\n",
       "  'capital': 1130,\n",
       "  'record': 1111,\n",
       "  'ready': 1100,\n",
       "  'store': 1073,\n",
       "  'stand': 1063,\n",
       "  'test': 1062,\n",
       "  'build': 1045,\n",
       "  'forward': 1034,\n",
       "  'previous': 1019,\n",
       "  'cover': 995,\n",
       "  'coffee': 963,\n",
       "  'explain': 953,\n",
       "  'plan': 952,\n",
       "  'blow': 940,\n",
       "  'hit': 937,\n",
       "  'thank': 932,\n",
       "  'picture': 928,\n",
       "  'dust': 920,\n",
       "  'dance': 917,\n",
       "  'sleep': 913,\n",
       "  'fact': 896,\n",
       "  'color': 888,\n",
       "  'place': 881,\n",
       "  'report': 878,\n",
       "  'teach': 865,\n",
       "  'dry': 835,\n",
       "  'restaurant': 811,\n",
       "  'clean': 808,\n",
       "  'trail': 791,\n",
       "  'complete': 788,\n",
       "  'text': 785,\n",
       "  'study': 785,\n",
       "  'imagine': 760},\n",
       " {'send': 711,\n",
       "  'create': 699,\n",
       "  'review': 681,\n",
       "  'jump': 673,\n",
       "  'perform': 662,\n",
       "  'allow': 659,\n",
       "  'pull': 643,\n",
       "  'rate': 640,\n",
       "  'throw': 635,\n",
       "  'flight': 623,\n",
       "  'ride': 619,\n",
       "  'reach': 619,\n",
       "  'temperature': 613,\n",
       "  'direct': 613,\n",
       "  'price': 606,\n",
       "  'finish': 602,\n",
       "  'catch': 583,\n",
       "  'smell': 573,\n",
       "  'cry': 564,\n",
       "  'schedule': 558,\n",
       "  'channel': 527,\n",
       "  'drop': 527,\n",
       "  'package': 524,\n",
       "  'label': 520,\n",
       "  'observe': 520,\n",
       "  'status': 503,\n",
       "  'condition': 501,\n",
       "  'count': 492,\n",
       "  'taste': 478,\n",
       "  'clock': 476,\n",
       "  'pack': 471,\n",
       "  'object': 462,\n",
       "  'share': 461,\n",
       "  'replace': 452,\n",
       "  'exchange': 445,\n",
       "  'laugh': 442,\n",
       "  'correct': 437,\n",
       "  'paint': 435,\n",
       "  'accept': 432,\n",
       "  'notice': 428,\n",
       "  'stick': 427,\n",
       "  'prepare': 427,\n",
       "  'float': 426,\n",
       "  'polish': 422,\n",
       "  'request': 415,\n",
       "  'reply': 410,\n",
       "  'guide': 407,\n",
       "  'stock': 405,\n",
       "  'smile': 404,\n",
       "  'receive': 403,\n",
       "  'apply': 401,\n",
       "  'interpret': 396,\n",
       "  'sing': 393,\n",
       "  'wake': 381,\n",
       "  'log': 370,\n",
       "  'format': 364,\n",
       "  'message': 362,\n",
       "  'cleaning': 358,\n",
       "  'forecast': 354,\n",
       "  'steam': 347,\n",
       "  'chart': 346,\n",
       "  'describe': 339,\n",
       "  'touch': 334,\n",
       "  'express': 333,\n",
       "  'appeal': 332,\n",
       "  'kick': 325,\n",
       "  'hang': 324,\n",
       "  'volume': 323,\n",
       "  'measure': 322,\n",
       "  'map': 321,\n",
       "  'ignore': 320,\n",
       "  'copy': 319,\n",
       "  'link': 319,\n",
       "  'develop': 318,\n",
       "  'kiss': 316,\n",
       "  'discuss': 308,\n",
       "  'lift': 298,\n",
       "  'treat': 296,\n",
       "  'resume': 292,\n",
       "  'rotate': 291,\n",
       "  'taxi': 290,\n",
       "  'protect': 288,\n",
       "  'capture': 288,\n",
       "  'grant': 288,\n",
       "  'challenge': 288,\n",
       "  'print': 281,\n",
       "  'swim': 279,\n",
       "  'slide': 279,\n",
       "  'shake': 272,\n",
       "  'pardon': 270,\n",
       "  'contrast': 264,\n",
       "  'alarm': 262,\n",
       "  'tape': 258,\n",
       "  'recommend': 258,\n",
       "  'photograph': 256,\n",
       "  'draw': 253,\n",
       "  'pat': 250,\n",
       "  'celebrate': 250,\n",
       "  'seek': 250,\n",
       "  'contest': 243})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neurobench.datasets.MSWC import generate_mswc_fscil_splits\n",
    "generate_mswc_fscil_splits(ROOT, [\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S = S2SProcessor(device, transpose=False)\n",
    "S2S._default_spec_kwargs[\"sample_rate\"] = 48000\n",
    "S2S._default_spec_kwargs[\"hop_length\"] = 240\n",
    "pre_proc = S2S.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_rate': 48000,\n",
       " 'n_mels': 20,\n",
       " 'n_fft': 512,\n",
       " 'f_min': 20,\n",
       " 'f_max': 4000,\n",
       " 'hop_length': 240}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2S._default_spec_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/p306982/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (256) may be set too high. Or, the value for `n_freqs` (1025) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "n_fft = 2048\n",
    "win_length = None\n",
    "hop_length = 240\n",
    "n_mels = 256\n",
    "n_mfcc = 256\n",
    "\n",
    "mfcc_transform = T.MFCC(\n",
    "    sample_rate=48000,\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs={\n",
    "        \"n_fft\": n_fft,\n",
    "        \"n_mels\": n_mels,\n",
    "        \"hop_length\": hop_length,\n",
    "        \"mel_scale\": \"htk\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(pre_train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = S2S.transform.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.1222e-09, 8.1323e-10, 8.3698e-10,  ..., 5.2586e-09,\n",
       "           1.8456e-09, 1.1892e-09],\n",
       "          [9.6105e-10, 1.2303e-09, 2.3467e-09,  ..., 3.0912e-09,\n",
       "           2.3288e-09, 1.6148e-09],\n",
       "          [2.9100e-09, 5.1502e-09, 7.8932e-09,  ..., 8.1274e-09,\n",
       "           1.1089e-08, 1.3257e-08],\n",
       "          ...,\n",
       "          [8.6771e-08, 6.4785e-08, 3.7651e-08,  ..., 5.8987e-08,\n",
       "           7.7212e-08, 1.0462e-07],\n",
       "          [1.1844e-07, 9.7498e-08, 7.5531e-08,  ..., 9.2827e-08,\n",
       "           1.0752e-07, 1.3343e-07],\n",
       "          [1.1271e-07, 1.1175e-07, 1.1668e-07,  ..., 1.4001e-07,\n",
       "           1.8513e-07, 2.2397e-07]]],\n",
       "\n",
       "\n",
       "        [[[2.0776e-08, 2.2499e-08, 2.9034e-08,  ..., 1.2042e-08,\n",
       "           4.4046e-09, 2.5336e-09],\n",
       "          [8.9449e-09, 5.8346e-09, 4.5939e-09,  ..., 4.7325e-09,\n",
       "           2.4665e-09, 1.7616e-09],\n",
       "          [3.9666e-09, 3.6041e-09, 3.3655e-09,  ..., 6.1593e-09,\n",
       "           1.6837e-08, 2.5383e-08],\n",
       "          ...,\n",
       "          [1.0682e-07, 1.2322e-07, 1.1922e-07,  ..., 5.4409e-08,\n",
       "           7.9825e-08, 9.9264e-08],\n",
       "          [1.5213e-07, 1.2950e-07, 9.6197e-08,  ..., 1.1199e-07,\n",
       "           1.6843e-07, 2.1408e-07],\n",
       "          [1.7189e-07, 1.7080e-07, 1.5669e-07,  ..., 2.0316e-07,\n",
       "           4.0705e-07, 5.4837e-07]]],\n",
       "\n",
       "\n",
       "        [[[1.4254e-08, 1.3543e-08, 1.2334e-08,  ..., 1.0127e-08,\n",
       "           1.0491e-08, 9.5612e-09],\n",
       "          [7.4349e-09, 5.0489e-09, 4.1780e-09,  ..., 3.3610e-09,\n",
       "           4.3281e-09, 5.0624e-09],\n",
       "          [3.8296e-09, 3.4636e-09, 3.1148e-09,  ..., 5.2789e-09,\n",
       "           6.2890e-09, 5.3895e-09],\n",
       "          ...,\n",
       "          [4.8800e-08, 6.2165e-08, 7.6259e-08,  ..., 7.4111e-08,\n",
       "           8.1684e-08, 9.1242e-08],\n",
       "          [8.1077e-08, 1.0994e-07, 1.8312e-07,  ..., 6.9854e-08,\n",
       "           9.9953e-08, 1.1607e-07],\n",
       "          [1.1827e-07, 1.1386e-07, 1.0769e-07,  ..., 1.2327e-07,\n",
       "           2.0480e-07, 2.4867e-07]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1.1599e-09, 3.1822e-09, 6.5036e-09,  ..., 8.2136e-09,\n",
       "           1.4013e-08, 1.5140e-08],\n",
       "          [6.2923e-09, 7.9597e-09, 9.5034e-09,  ..., 1.3518e-08,\n",
       "           2.0691e-08, 2.3043e-08],\n",
       "          [3.2809e-09, 3.9937e-09, 3.6732e-09,  ..., 4.6127e-09,\n",
       "           2.6509e-09, 1.5078e-09],\n",
       "          ...,\n",
       "          [1.3260e-07, 9.6678e-08, 4.4503e-08,  ..., 7.7333e-08,\n",
       "           4.6613e-08, 3.0038e-08],\n",
       "          [3.3418e-07, 2.6161e-07, 1.6375e-07,  ..., 1.0219e-07,\n",
       "           9.3648e-08, 8.4090e-08],\n",
       "          [2.2666e-07, 2.5406e-07, 2.5741e-07,  ..., 2.6622e-07,\n",
       "           2.0846e-07, 1.5196e-07]]],\n",
       "\n",
       "\n",
       "        [[[7.4092e-10, 9.0413e-10, 1.5561e-09,  ..., 6.6337e-09,\n",
       "           7.3408e-09, 8.0062e-09],\n",
       "          [3.5837e-09, 3.6736e-09, 3.1319e-09,  ..., 6.3828e-09,\n",
       "           2.8113e-09, 2.3178e-09],\n",
       "          [4.7919e-09, 4.2944e-09, 3.8004e-09,  ..., 5.1735e-09,\n",
       "           5.3145e-09, 4.7645e-09],\n",
       "          ...,\n",
       "          [6.9828e-08, 7.4179e-08, 8.2791e-08,  ..., 4.5465e-08,\n",
       "           5.2177e-08, 5.9033e-08],\n",
       "          [2.7856e-07, 2.2557e-07, 1.3933e-07,  ..., 9.2976e-08,\n",
       "           6.3559e-08, 5.2299e-08],\n",
       "          [2.4493e-07, 3.3471e-07, 4.4863e-07,  ..., 1.9284e-07,\n",
       "           1.5177e-07, 1.2667e-07]]],\n",
       "\n",
       "\n",
       "        [[[7.2218e-10, 2.1645e-09, 7.4216e-09,  ..., 4.8950e-09,\n",
       "           4.2065e-09, 4.3155e-09],\n",
       "          [5.4801e-10, 2.5232e-09, 5.5277e-09,  ..., 1.6567e-09,\n",
       "           3.5879e-09, 4.7898e-09],\n",
       "          [3.4383e-09, 5.6997e-09, 9.5370e-09,  ..., 1.5009e-08,\n",
       "           1.5699e-08, 1.3227e-08],\n",
       "          ...,\n",
       "          [7.6197e-08, 7.3722e-08, 7.0858e-08,  ..., 6.9112e-08,\n",
       "           7.8009e-08, 8.4693e-08],\n",
       "          [6.1902e-08, 9.3162e-08, 1.5062e-07,  ..., 7.3259e-08,\n",
       "           8.8296e-08, 1.0040e-07],\n",
       "          [9.8238e-08, 1.1772e-07, 1.5267e-07,  ..., 1.9735e-07,\n",
       "           2.5632e-07, 3.0520e-07]]]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(data[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike, target = S2S((data[0].to(device), data[1].to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 601, 20])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spike.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 201])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = base_train_set[0][0]\n",
    "mfcc = mfcc_transform(data)\n",
    "mfcc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.9695e+02, -5.9704e+02, -5.9712e+02,  ..., -5.9683e+02,\n",
       "          -5.9659e+02, -5.9648e+02],\n",
       "         [-7.4492e-01, -6.2521e-01, -5.1344e-01,  ..., -9.3668e-01,\n",
       "          -1.2713e+00, -1.4302e+00],\n",
       "         [ 7.0271e-01,  5.8958e-01,  4.8761e-01,  ...,  9.2732e-01,\n",
       "           1.2569e+00,  1.4129e+00],\n",
       "         ...,\n",
       "         [-1.2441e-01, -9.6221e-02, -4.0581e-02,  ..., -1.3851e-02,\n",
       "          -1.2847e-02, -3.8621e-03],\n",
       "         [ 8.5055e-02,  6.5515e-02,  2.7330e-02,  ...,  8.0706e-03,\n",
       "           7.6974e-03,  1.6639e-03],\n",
       "         [-4.3595e-02, -3.3596e-02, -1.4172e-02,  ..., -4.0992e-03,\n",
       "          -4.0109e-03, -9.8432e-04]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mfcc==0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 133/195 [00:46<00:21,  2.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m zero_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m,))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(pre_train_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(base_train_set)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mBATCH_SIZE):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     mfcc \u001b[39m=\u001b[39m mfcc_transform(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     zero_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39msum(mfcc\u001b[39m==\u001b[39m\u001b[39m0.0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:730\u001b[0m, in \u001b[0;36mMFCC.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    723\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39m        Tensor: specgram_mel_db of size (..., ``n_mfcc``, time).\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 730\u001b[0m     mel_specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMelSpectrogram(waveform)\n\u001b[1;32m    731\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_mels:\n\u001b[1;32m    732\u001b[0m         log_offset \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:650\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    643\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspectrogram(waveform)\n\u001b[1;32m    651\u001b[0m     mel_specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    652\u001b[0m     \u001b[39mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mspectrogram(\n\u001b[1;32m    111\u001b[0m         waveform,\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad,\n\u001b[1;32m    113\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow,\n\u001b[1;32m    114\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_fft,\n\u001b[1;32m    115\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhop_length,\n\u001b[1;32m    116\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwin_length,\n\u001b[1;32m    117\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower,\n\u001b[1;32m    118\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized,\n\u001b[1;32m    119\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter,\n\u001b[1;32m    120\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_mode,\n\u001b[1;32m    121\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monesided,\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/functional/functional.py:147\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m power \u001b[39m==\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m spec_f\u001b[39m.\u001b[39mabs()\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m spec_f\u001b[39m.\u001b[39;49mabs()\u001b[39m.\u001b[39mpow(power)\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m spec_f\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "zero_count = torch.zeros((1,))\n",
    "for batch_idx, (data, target) in tqdm(enumerate(pre_train_loader), total=len(base_train_set)//BATCH_SIZE):\n",
    "    mfcc = mfcc_transform(data)\n",
    "    zero_count +=torch.sum(mfcc==0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.isinf(mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5_Feature(nn.Module):\n",
    "    def __init__(self, n_input=1, stride=16, n_channel=32, drop =False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=8, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "\n",
    "        # self.avg_pool = nn.AvgPool1d(64)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(self.bn1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(self.bn2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.act3(self.bn3(x))\n",
    "        x = self.drop3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.act4(self.bn4(x))\n",
    "        x = self.drop4(x)\n",
    "        x = self.pool4(x)\n",
    "        # x = self.avg_pool(x)\n",
    "        # x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "beta = 0.9\n",
    "num_steps = 50\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import cat, mul\n",
    "import collections\n",
    "\n",
    "def remove_sequential(network, all_layers):\n",
    "\n",
    "    for name, layer in network.named_children():\n",
    "        if isinstance(layer, nn.Sequential): # if sequential layer, apply recursively to layers in sequential layer\n",
    "            #print(layer)\n",
    "            remove_sequential(layer, all_layers)\n",
    "        else: # if leaf node, add it to list\n",
    "            # print(layer)\n",
    "            all_layers.append((name,layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNN_features(nn.Module):\n",
    "    def __init__(self, n_input=1, stride=16, n_channel=32, input_kernel=80, pool_kernel=4, drop=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=input_kernel, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.drop1 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool1 = nn.MaxPool1d(pool_kernel)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.drop2 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool2 = nn.MaxPool1d(pool_kernel)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.drop3 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool3 = nn.MaxPool1d(pool_kernel)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.lif4 = snn.Leaky(beta=beta, spike_grad=spike_grad)\n",
    "        self.drop4 = nn.Dropout(p=0.2) if drop else nn.Identity()\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "\n",
    "        # self.avg_pool = nn.AvgPool1d(64)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x, mem1 = self.lif1(self.bn1(x), mem1)\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x, mem2 = self.lif2(self.bn2(x), mem2)\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x, mem3 = self.lif3(self.bn3(x), mem3)\n",
    "        x = self.drop3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x, mem4 = self.lif4(self.bn4(x), mem4)\n",
    "        x = self.drop4(x)\n",
    "        x = self.pool4(x)\n",
    "        # x = self.avg_pool(x)\n",
    "        # x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class SCNN(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=16, \n",
    "                 n_channel=32, load_model=None, output= None, input_kernel=80, pool_kernel=4, drop=False, latent_layer_num=100):\n",
    "        \"\"\"Taken from: https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if load_model:\n",
    "            all_layers = []\n",
    "            # all_seq = nn.Sequential(load_model.lat_features, load_model.end_features)\n",
    "            remove_sequential(load_model.lat_features, all_layers)\n",
    "\n",
    "        else:\n",
    "            features = SCNN_features(n_input, stride, n_channel, input_kernel, pool_kernel, drop)\n",
    "            \n",
    "            all_layers = []\n",
    "            remove_sequential(features, all_layers)\n",
    "\n",
    "        lat_list = []\n",
    "        end_list = []\n",
    "\n",
    "        for i, layer in enumerate(all_layers):\n",
    "            if i <= latent_layer_num:\n",
    "                lat_list.append(layer)\n",
    "            else:\n",
    "                end_list.append(layer)\n",
    "\n",
    "        self.lat_features = nn.Sequential(collections.OrderedDict(lat_list))\n",
    "        self.end_features = nn.Sequential(collections.OrderedDict(end_list))\n",
    "\n",
    "        if load_model:\n",
    "            self.output = load_model.output\n",
    "        else:\n",
    "            self.output = nn.Linear(2 * n_channel, n_output, bias=False)\n",
    "\n",
    "    def forward(self, x, latent_input=None, return_lat_acts=False):\n",
    "\n",
    "        utils.reset(self.lat_features)\n",
    "        utils.reset(self.end_features)\n",
    "        # mem1 = self.lif1.init_leaky()\n",
    "        # mem2 = self.lif2.init_leaky()\n",
    "        # mem3 = self.lif3.init_leaky()\n",
    "        # mem4 = self.lif4.init_leaky()\n",
    "\n",
    "        num_steps = x.shape[1]\n",
    "\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            orig_acts = self.lat_features(x[step])\n",
    "            if latent_input is not None:\n",
    "                lat_acts = cat((orig_acts, latent_input), 0)\n",
    "            else:\n",
    "                lat_acts = orig_acts\n",
    "\n",
    "            logits = self.end_features(lat_acts)\n",
    "            logits = F.avg_pool1d(logits, logits.shape[-1])\n",
    "            logits = logits.permute(0, 2, 1)\n",
    "\n",
    "            outputs = self.output(logits)\n",
    "\n",
    "        # masked_outputs = torch.mul(self.mask, outputs)\n",
    "\n",
    "        if return_lat_acts:\n",
    "            return outputs, orig_acts\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNN(nn.Module):\n",
    "    def __init__(self,load_model=None, drop=False, latent_layer_num=100):\n",
    "        \"\"\"Taken from: https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if load_model:\n",
    "            all_layers = []\n",
    "            # all_seq = nn.Sequential(load_model.lat_features, load_model.end_features)\n",
    "            remove_sequential(load_model.lat_features, all_layers)\n",
    "\n",
    "        else:\n",
    "            net = nn.Sequential(\n",
    "                # nn.Flatten(),\n",
    "                nn.Linear(20, 256),\n",
    "                snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                nn.Linear(256, 256),\n",
    "                snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                nn.Linear(256, 256),\n",
    "                snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True),\n",
    "                nn.Linear(256, 256),\n",
    "                snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True)\n",
    "            )\n",
    "            all_layers = []\n",
    "            remove_sequential(net, all_layers)\n",
    "\n",
    "        lat_list = []\n",
    "        end_list = []\n",
    "\n",
    "        for i, layer in enumerate(all_layers):\n",
    "            if i <= latent_layer_num:\n",
    "                lat_list.append(layer)\n",
    "            else:\n",
    "                end_list.append(layer)\n",
    "\n",
    "        self.lat_features = nn.Sequential(collections.OrderedDict(lat_list))\n",
    "        self.end_features = nn.Sequential(collections.OrderedDict(end_list))\n",
    "\n",
    "        self.output = nn.Linear(256, 200, bias=False)\n",
    "        self.output_neurons = snn.Leaky(beta=beta, spike_grad=spike_grad, init_hidden=True, output=True)\n",
    "\n",
    "    def forward(self, x, latent_input=None, return_lat_acts=False):\n",
    "\n",
    "        utils.reset(self.lat_features)\n",
    "        utils.reset(self.end_features)\n",
    "        utils.reset(self.output_neurons)\n",
    "        softmax_output = torch.zeros(x.size(0), 200).to(device)\n",
    "        # mem1 = self.lif1.init_leaky()\n",
    "        # mem2 = self.lif2.init_leaky()\n",
    "        # mem3 = self.lif3.init_leaky()\n",
    "        # mem4 = self.lif4.init_leaky()\n",
    "        # mem = []\n",
    "\n",
    "        num_steps = x.shape[1]\n",
    "\n",
    "        for step in range(num_steps):\n",
    "\n",
    "            orig_acts = self.lat_features(x[:,step])\n",
    "            if latent_input is not None:\n",
    "                lat_acts = cat((orig_acts, latent_input), 0)\n",
    "            else:\n",
    "                lat_acts = orig_acts\n",
    "\n",
    "            logits = self.end_features(lat_acts)\n",
    "            # logits = F.avg_pool1d(logits, logits.shape[-1])\n",
    "            # logits = logits.permute(0, 2, 1)\n",
    "\n",
    "            outputs = self.output(logits)\n",
    "            _, mem_out = self.output_neurons(outputs)\n",
    "\n",
    "            softmax_output += F.softmax(mem_out, dim=1)\n",
    "\n",
    "        if return_lat_acts:\n",
    "            return softmax_output, orig_acts\n",
    "        else:\n",
    "            return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 200])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(spike)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0050, 3.0050, 3.0050,  ..., 3.0050, 3.0050, 3.0050],\n",
       "        [3.0050, 3.0050, 3.0050,  ..., 3.0050, 3.0050, 3.0050],\n",
       "        [3.0050, 3.0050, 3.0050,  ..., 3.0050, 3.0050, 3.0050],\n",
       "        ...,\n",
       "        [3.0050, 3.0050, 3.0050,  ..., 3.0050, 3.0050, 3.0050],\n",
       "        [3.0050, 3.0050, 3.0050,  ..., 3.0050, 3.0050, 3.0050],\n",
       "        [3.0050, 3.0050, 3.0050,  ..., 3.0050, 3.0050, 3.0050]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5_Feature(256, 2, 128, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = torch.load(os.path.join(ROOT, \"model_ref\"), map_location=device)\n",
    "model = M5(n_input=1, n_output=200, load_model= load_model, latent_layer_num=9).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1745408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in load_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M5(\n",
       "  (lat_features): Sequential(\n",
       "    (conv1): Conv1d(256, 256, kernel_size=(4,), stride=(2,))\n",
       "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (drop1): Dropout(p=0.2, inplace=False)\n",
       "    (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (drop2): Dropout(p=0.2, inplace=False)\n",
       "    (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (end_features): Sequential(\n",
       "    (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "    (drop3): Dropout(p=0.2, inplace=False)\n",
       "    (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
       "    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act4): ReLU()\n",
       "    (drop4): Dropout(p=0.2, inplace=False)\n",
       "    (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (output): Linear(in_features=512, out_features=200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M5(\n",
       "  (lat_features): Sequential(\n",
       "    (conv1): Conv1d(256, 256, kernel_size=(8,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act1): ReLU()\n",
       "    (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
       "    (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act2): ReLU()\n",
       "    (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
       "    (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act3): ReLU()\n",
       "    (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
       "    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act4): ReLU()\n",
       "    (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (end_features): Sequential()\n",
       "  (output): Linear(in_features=512, out_features=200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "conv1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "bn2.weight\n",
      "bn2.bias\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "bn3.weight\n",
      "bn3.bias\n",
      "conv4.weight\n",
      "conv4.bias\n",
      "bn4.weight\n",
      "bn4.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9530, 0.0000, 0.0000,  ..., 0.4128, 0.0000, 0.0000],\n",
       "         [0.3260, 0.5191, 0.7440,  ..., 0.5860, 1.0364, 0.8143],\n",
       "         [0.2114, 0.6727, 0.6269,  ..., 1.1413, 0.7937, 0.9362],\n",
       "         ...,\n",
       "         [1.9893, 0.6086, 0.4245,  ..., 0.4697, 0.4444, 0.0318],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0509, 0.5623,  ..., 0.6040, 0.6049, 1.2391]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (256x1x4). Calculated output size: (256x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m avg_pool \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mAvgPool1d(\u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m feature \u001b[39m=\u001b[39m avg_pool(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m feature\u001b[39m.\u001b[39msize()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/pooling.py:559\u001b[0m, in \u001b[0;36mAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mavg_pool1d(\n\u001b[1;32m    560\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    561\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcount_include_pad)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (256x1x4). Calculated output size: (256x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "avg_pool = nn.AvgPool1d(10)\n",
    "feature = avg_pool(data)\n",
    "feature.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
